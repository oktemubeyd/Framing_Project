{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GB9mS0Cki2_4"
      },
      "source": [
        "## Import Necessary Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZVsMiLTocR2l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de9d294f-7dfb-4593-9db7-dd00188c2715"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: optuna in /usr/local/lib/python3.11/dist-packages (4.3.0)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (1.15.2)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from optuna) (6.9.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.40)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.13.2)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install optuna\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "from torch.optim import AdamW\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "import numpy as np\n",
        "import re\n",
        "import optuna"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1yAUeoki9Rj"
      },
      "source": [
        "## Importing the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "UI85nXKOcZpN"
      },
      "outputs": [],
      "source": [
        "df_train_and_val = pd.read_excel(\"train_and_val_set_economic_relationship.xlsx\")[['content', 'Economic_Relationship']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "6SQfviFrcaZW",
        "outputId": "3300c3d9-620f-4aac-8fac-ed4e438a3e37"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               content  Economic_Relationship\n",
              "0    Çin'den Kazakistan'a Kuşak ve Yol Girişimi tek...                      1\n",
              "1    Çin, ABD'ye gidecek vatandaşlarını uyardı \\n  ...                      0\n",
              "2    Cenevre Fuarı öncesi \\n                    Oto...                      0\n",
              "3    “Dönüşmezsek ana pazarları Çin ve Hindistan'a ...                      1\n",
              "4    Çin nadir metallerin işlenmesine yönelik tekno...                      0\n",
              "..                                                 ...                    ...\n",
              "398  Cumhurbaşkanı Erdoğan, Çin Devlet Başkanı Şi i...                      1\n",
              "399  Güney Çin Denizi'nde gerilim: Baltalı ve bıçak...                      0\n",
              "400  Bakan Fidan’ın Çin ziyaretinin eko-politiği Tü...                      1\n",
              "401  Apple baskıya dayanamadı: WhatsApp ve Threads'...                      0\n",
              "402  CES’te Çin istilası Dünyanın en büyük tüketici...                      0\n",
              "\n",
              "[403 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e36cbb77-8036-4062-9911-1944646f5aeb\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>content</th>\n",
              "      <th>Economic_Relationship</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Çin'den Kazakistan'a Kuşak ve Yol Girişimi tek...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Çin, ABD'ye gidecek vatandaşlarını uyardı \\n  ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Cenevre Fuarı öncesi \\n                    Oto...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>“Dönüşmezsek ana pazarları Çin ve Hindistan'a ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Çin nadir metallerin işlenmesine yönelik tekno...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>398</th>\n",
              "      <td>Cumhurbaşkanı Erdoğan, Çin Devlet Başkanı Şi i...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>399</th>\n",
              "      <td>Güney Çin Denizi'nde gerilim: Baltalı ve bıçak...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>400</th>\n",
              "      <td>Bakan Fidan’ın Çin ziyaretinin eko-politiği Tü...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>401</th>\n",
              "      <td>Apple baskıya dayanamadı: WhatsApp ve Threads'...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>402</th>\n",
              "      <td>CES’te Çin istilası Dünyanın en büyük tüketici...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>403 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e36cbb77-8036-4062-9911-1944646f5aeb')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e36cbb77-8036-4062-9911-1944646f5aeb button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e36cbb77-8036-4062-9911-1944646f5aeb');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-f116f0d0-d1ca-4fb1-8e89-5a3348ca2e61\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f116f0d0-d1ca-4fb1-8e89-5a3348ca2e61')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-f116f0d0-d1ca-4fb1-8e89-5a3348ca2e61 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_7e27f1c5-dc26-4d39-ae11-34b69e0e3eb0\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_train_and_val')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_7e27f1c5-dc26-4d39-ae11-34b69e0e3eb0 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_train_and_val');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_train_and_val",
              "summary": "{\n  \"name\": \"df_train_and_val\",\n  \"rows\": 403,\n  \"fields\": [\n    {\n      \"column\": \"content\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 403,\n        \"samples\": [\n          \"\\u00c7in heyeti, Be\\u015fikta\\u015f M\\u00fczesi'ni gezdi \\n                \\u00c7in'in Ankara B\\u00fcy\\u00fckel\\u00e7isi Yu Hongyang ve beraberindeki heyet, Be\\u015fikta\\u015f-Trabzonspor ma\\u00e7\\u0131 \\u00f6ncesinde Be\\u015fikta\\u015f M\\u00fczesi'ni gezdi._x000D_\\nBe\\u015fikta\\u015f Kul\\u00fcb\\u00fc ad\\u0131na \\u00c7in heyetini, y\\u00f6neticiler Ahmet Nur \\u00c7ebi, Hakan \\u00d6zk\\u00f6se, Deniz Atalay ile Ahmet \\u00dcrkmezgil a\\u011f\\u0131rlad\\u0131. Gezide baz\\u0131 \\u00c7inli gazeteciler de yer ald\\u0131._x000D_\\nGeziyle ilgili bas\\u0131n mensuplar\\u0131na a\\u00e7\\u0131klamada bulunan Be\\u015fikta\\u015f Kul\\u00fcb\\u00fc \\u0130kinci Ba\\u015fkan\\u0131 Ahmet Nur \\u00c7ebi, \\u00c7in ile ili\\u015fkileri geli\\u015ftirmek istediklerini belirterek, \\\"Bug\\u00fcn \\u00c7in, yar\\u0131n Pakistan olur. \\u00c7in'in milli g\\u00fcn\\u00fc, b\\u00f6yle bir g\\u00fcnde onlar\\u0131 a\\u011f\\u0131rlamak b\\u00fcy\\u00fck mutluluk.\\\" dedi._x000D_\\n\\u00c7in halk\\u0131n\\u0131n Be\\u015fikta\\u015f'\\u0131 sevmelerini istedi\\u011fini kaydeden \\u00c7ebi, \\\"B\\u00fct\\u00fcn \\u00c7in halk\\u0131na kendimizi sevdirmek istiyoruz. Formalar\\u0131m\\u0131z\\u0131 als\\u0131nlar, bizi sevsinler. Ma\\u00e7lar\\u0131m\\u0131z\\u0131 izlerken 'Ben Be\\u015fikta\\u015fl\\u0131y\\u0131m' desinler. \\u00c7in halk\\u0131 sempatiktir, Be\\u015fikta\\u015f da herkes i\\u00e7in sempatik.\\\" ifadelerini kulland\\u0131._x000D_\\nB\\u00fcy\\u00fckel\\u00e7i Yu Hongyang ise \\u00c7in'in milli g\\u00fcn\\u00fcn\\u00fc Be\\u015fikta\\u015f ile kutlad\\u0131klar\\u0131n\\u0131 vurgulayarak, \\\"Be\\u015fikta\\u015f heyetine bize bu f\\u0131rsat\\u0131 verdi\\u011fi i\\u00e7in te\\u015fekk\\u00fcr ediyorum. T\\u00fcrk halk\\u0131 \\u00c7in halk\\u0131na dostane davran\\u0131yor. Bu ili\\u015fkiler, Be\\u015fikta\\u015f Kul\\u00fcb\\u00fcn\\u00fcn \\u00c7in ile ili\\u015fkilerine \\u00f6nem verdi\\u011fini g\\u00f6steriyor. \\u00c7in ile T\\u00fcrkiye aras\\u0131ndaki dostluk uzun ge\\u00e7mi\\u015fe dayan\\u0131yor.\\\" de\\u011ferlendirmesinde bulundu._x000D_\\nSpor alan\\u0131nda da ili\\u015fkileri geli\\u015ftirmek istediklerini anlatan Hongyang, \\u015funlar\\u0131 s\\u00f6yledi:_x000D_\\n\\\"K\\u0131sa s\\u00fcre \\u00f6nce Be\\u015fikta\\u015f'\\u0131 \\u00c7in'de a\\u011f\\u0131rlad\\u0131k. Bundan memnuniyet duyuyoruz. Bu seferki ziyaret \\u00e7ok ba\\u015far\\u0131l\\u0131 oldu. Sadece spor alan\\u0131ndaki i\\u015f birli\\u011fi i\\u00e7in de\\u011fil ayn\\u0131 zamanda halklar aras\\u0131nda dostlu\\u011fu da peki\\u015ftirdi. Be\\u015fikta\\u015f bunun i\\u00e7in \\u00e7ok \\u00f6nemli k\\u00f6pr\\u00fc g\\u00f6rev \\u00fcstlendi. \\u00c7in taraf\\u0131 da Be\\u015fikta\\u015f ile i\\u015f birli\\u011fi yapmaya haz\\u0131rd\\u0131r.\\\"_x000D_\\nHongyang, Be\\u015fikta\\u015fl\\u0131l\\u0131\\u011f\\u0131 ile bilinen \\u0130ngiltere'nin Ankara B\\u00fcy\\u00fckel\\u00e7isi Richard Moore'nin kul\\u00fcbe \\u00fcye oldu\\u011fu hat\\u0131rlatarak y\\u00f6neltilen, kendisinin de b\\u00f6yle bir giri\\u015fimde bulunup bulunmayaca\\u011f\\u0131 sorusu \\u00fczerine, \\\"Bu Be\\u015fikta\\u015f'\\u0131n karar\\u0131na ba\\u011fl\\u0131.\\\" derken, Ahmet Nur \\u00c7ebi, \\\"Be\\u015fikta\\u015f'a ho\\u015f geldiniz.\\\" diyoruz' ifadesini kulland\\u0131. Be\\u015fikta\\u015f Kul\\u00fcb\\u00fc Genel Sekreteri Ahmet \\u00dcrkmezgil de \\u00fcyelik kart\\u0131n\\u0131 en k\\u0131sa s\\u00fcrede kendisine vereceklerini dile getirdi.\\n            \",\n          \"T\\u00fcrkiye ile \\u00c7in'in ticaret hedefi 100 milyar dolar \\n                \\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0\\u00a0Ge\\u00e7en y\\u0131l \\u00c7in ile 20 milyar dolarl\\u0131k ticari ili\\u015fki ger\\u00e7ekle\\u015ftiren T\\u00fcrkiye, 2015 y\\u0131l\\u0131nda bu rakam\\u0131 ikiye katlayacak. 10 y\\u0131l i\\u00e7erisinde ise iki \\u00fclke aras\\u0131ndaki ticaret 100 milyar dolar\\u0131 bulacak. Ge\\u00e7en y\\u0131l ger\\u00e7ekle\\u015ftirdikleri \\u00c7in ziyareti s\\u0131ras\\u0131nda \\u00fclkedeki potansiyeli yerinde inceleme f\\u0131rsat\\u0131 bulan T\\u00fcrkiye \\u0130\\u015fadamlar\\u0131 ve Sanayiciler Konfederasyonu (TUSKON), bu potansiyeli de\\u011ferlendirmek amac\\u0131 ile T\\u00fcrkiye - \\u00c7in \\u0130kili \\u0130\\u015f G\\u00f6r\\u00fc\\u015fmeleri organizasyonu d\\u00fczenledi.\\u00c7in'in Jilin Eyaleti'nden 30 i\\u015fadam\\u0131n\\u0131 \\u0130stanbul'da a\\u011f\\u0131rland\\u0131\\u011f\\u0131 organizasyonda T\\u00fcrkiye ile \\u00c7in aras\\u0131nda ticari hedefler belirlendi. Buna g\\u00f6re\\u00a0 ge\\u00e7en y\\u0131l 20 milyar dolarl\\u0131k ticaret ger\\u00e7ekle\\u015ftiren iki \\u00fclke, 2015 y\\u0131l\\u0131nda 40 milyar dolarl\\u0131k ticaret yapmay\\u0131 hedefliyor. T\\u00fcrkiye ve \\u00c7in'in 10 y\\u0131l sonraki ticari ili\\u015fki hedefi ise 100 milyardolar.\\\"\\u00c7in \\u015eangay ve Pekin'den ibaret de\\u011fil\\\"Toplant\\u0131da konu\\u015fan TUSKON Ba\\u015fkan R\\u0131zanur Meral, \\u00c7in'in d\\u00fcnyan\\u0131n en b\\u00fcy\\u00fck ekonomilerinden biri oldu\\u011funu belirterek, \\\"\\u00c7in d\\u00fcnyan\\u0131n en \\u00e7ok mal satan birinci, en \\u00e7ok mal alan ise ikinci \\u00fclkesi konumunda. \\u00c7in ayr\\u0131ca 3.3 trilyon dolarl\\u0131k d\\u00f6viz rezervi ile de \\u00e7ok zengin bir \\u00fclke. Ayn\\u0131 zamanda d\\u00fcnyadaki \\u00f6nemli g\\u00fc\\u00e7lerden de birisi. Biz \\u00c7in ile ili\\u015fkilerimize \\u00e7ok \\u00f6nem veriyoruz ve ayn\\u0131 ilgiyi \\u00c7in'den de g\\u00f6r\\u00fcyoruz\\\"dedi. T\\u00fcrkiye'nin 375 milyar dolarl\\u0131k d\\u0131\\u015f ticaretimiz ile d\\u00fcnyan\\u0131n 17. b\\u00fcy\\u00fck ekonomisi oldu\\u011funa dikkat \\u00e7eken R\\u0131zanur Meral, \\\"\\u00c7in kadar b\\u00fcy\\u00fck de\\u011filiz ama, 54 Afrika \\u00fclkesinin bu \\u00fclkeye yapt\\u0131\\u011f\\u0131 ithalat\\u0131n yar\\u0131s\\u0131kadar \\u00c7in'den ithalat ger\\u00e7ekle\\u015ftiriyoruz. Bu da iki \\u00fclke aras\\u0131ndaki potansiyelin y\\u00fcksek oldu\\u011funu ve bunun daha da artmas\\u0131 gerekti\\u011fini ortaya koyuyor. Ayr\\u0131ca \\u00c7in'in sadece Pekin veya \\u015eangay'dan ibaretolmad\\u0131\\u011f\\u0131n\\u0131n da fark\\u0131nday\\u0131z. Bu nedenle Jilin ile de ili\\u015fkilerimizi geli\\u015ftirmek istiyoruz\\\" diye konu\\u015ftu.4.3 milyar dolarl\\u0131k kontrat imzaland\\u0131\\u00c7in Halk Cumhuriyeti Jilin Eyaleti Bakan\\u0131 Wan Fu Yi ise, toplant\\u0131da Jilin'in ekonomisinden s\\u00f6z etti. 187.000 kilometrekarelik y\\u00fcz \\u00f6l\\u00e7\\u00fcme sahip olan eyaletden elde edilen gelirin 170 milyar dolar civar\\u0131nda oldu\\u011funu a\\u00e7\\u0131klayan Wan Fu Yi, \\\"Bu rakam \\u00c7in'in y\\u00fczde 2'sini olu\\u015fturuyor. Eyaletimizde sanayi ve tar\\u0131m \\u00e7ok geli\\u015fmi\\u015ftir. \\u00c7in'in tar\\u0131m b\\u00f6lgesi say\\u0131l\\u0131r\\u0131z. Uzakdo\\u011fu'nun en b\\u00fcy\\u00fck h\\u0131zl\\u0131 tren \\u015firketi Jilin'dedir. Petrokimya alan\\u0131nda da \\u00f6nemli \\u015firketlerimiz var. Petrol rezervleri konusunda da \\u00f6nemli bir konumday\\u0131z. Otomotiv sekt\\u00f6r\\u00fc de ayn\\u0131 \\u015fekilde. Sadece tar\\u0131mdaki \\u00fcretimimiz 130 milyon ton civar\\u0131nda\\\" dedi. Ticari ili\\u015fkileri geli\\u015ftirmek amac\\u0131 ile T\\u00fcrkiye'ye geldiklerini ifade eden \\u0130stanbul Ba\\u015fkonsoloslu\\u011fu \\u00c7in Ticaret Ate\\u015fesi Li Jiang ise iki \\u00fclkenin ekonomik ve siyasi a\\u00e7\\u0131dan h\\u0131zla geli\\u015fti\\u011fine dikkat \\u00e7ekti.2010 y\\u0131l\\u0131nda iki \\u00fclke aras\\u0131ndaki ticaretin 2000 y\\u0131l\\u0131na g\\u00f6re 12 kat artt\\u0131\\u011f\\u0131n\\u0131 vurgulayan Li Jiang, \\\"Ge\\u00e7en y\\u0131l kar\\u015f\\u0131l\\u0131kl\\u0131 olarak 4.3 milyar dolarl\\u0131k yat\\u0131r\\u0131m kontratlar\\u0131 imzaland\\u0131. Bunun yan\\u0131 s\\u0131ra ge\\u00e7en y\\u0131l T\\u00fcrkiye ile \\u00c7in aras\\u0131ndaki ticaret hacmi 20 milyar dolar idi. 2015 y\\u0131l\\u0131nda bu rakam\\u0131 40 milyar dolara, 10 y\\u0131l i\\u00e7erisinde de 100 milyar dolara \\u00e7\\u0131karmay\\u0131 hedefliyoruz\\\" dedi.\\n            \",\n          \"Erdo\\u011fan, Cinping ile g\\u00f6r\\u00fc\\u015ft\\u00fc \\n                \\u0130STANBUL - Ba\\u015fbakan Recep Tayyip Erdo\\u011fan, \\u00c7in Halk Cumhuriyeti Devlet Ba\\u015fkan Yard\\u0131mc\\u0131s\\u0131 \\u015ei Cinping ile g\\u00f6r\\u00fc\\u015ft\\u00fc.Erdo\\u011fan, Be\\u015fikta\\u015f'taki Ba\\u015fbakanl\\u0131k \\u00c7al\\u0131\\u015fma Ofisi'nde bir araya geldi\\u011fi \\u00c7in Halk Cumhuriyeti Devlet Ba\\u015fkan Yard\\u0131mc\\u0131s\\u0131 \\u015ei Cinping ile \\u00f6zel bir g\\u00f6r\\u00fc\\u015fme ger\\u00e7ekle\\u015ftirdi.Daha sonra heyetler aras\\u0131 g\\u00f6r\\u00fc\\u015fmenin yap\\u0131laca\\u011f\\u0131 toplant\\u0131da Ba\\u015fbakan Erdo\\u011fan'a, Ba\\u015fbakan Yard\\u0131mc\\u0131s\\u0131 Ali Babacan, Ekonomi Bakan\\u0131 Zafer \\u00c7a\\u011flayan, Kalk\\u0131nma Bakan\\u0131 Cevdet Y\\u0131lmaz, Ula\\u015ft\\u0131rma, Denizcilik ve Haberle\\u015fme Bakan\\u0131 Binali Y\\u0131ld\\u0131r\\u0131m, AK Parti D\\u0131\\u015f \\u0130li\\u015fkilerden Sorumlu Genel Ba\\u015fkan Yard\\u0131mc\\u0131s\\u0131 ve Adana Milletvekili \\u00d6mer \\u00c7elik, D\\u0131\\u015fi\\u015fleri Bakanl\\u0131\\u011f\\u0131 M\\u00fcste\\u015far\\u0131 Feridun Sinirlio\\u011flu ve T\\u00fcrkiye'nin Pekin B\\u00fcy\\u00fckel\\u00e7isi Murat Salim Esenli e\\u015flik etti.Ba\\u015fbakanl\\u0131k kaynaklar\\u0131ndan al\\u0131nan bilgiye g\\u00f6re g\\u00f6r\\u00fc\\u015fmede, \\u00c7in'in T\\u00fcrkiye'deki \\u00f6nemli projelerle ilgilendi\\u011fi ve iki \\u00fclke aras\\u0131ndaki ekonomik i\\u015fbirli\\u011finin daha da yo\\u011funla\\u015ft\\u0131r\\u0131laca\\u011f\\u0131 kaydedildi.\\u00c7inli yetkililerin, T\\u00fcrk ekonomisinin performans\\u0131n\\u0131 dikkat \\u00e7ekici bulduklar\\u0131n\\u0131 dile getirdikleri g\\u00f6r\\u00fc\\u015fmede, T\\u00fcrkiye ile \\u00c7in i\\u015fbirli\\u011fine \\u00f6nem atfettikleri bildirildi.G\\u00f6r\\u00fc\\u015fme s\\u0131ras\\u0131nda \\u015ei Cinping'in Ba\\u015fbakan Erdo\\u011fan'\\u0131 \\u00c7in'e resmi ziyaret i\\u00e7in davet etti\\u011fi ve Erdo\\u011fan'\\u0131n da bu davete olumlu cevap vererek gelecek aylarda \\u00c7in'i ziyaret edece\\u011fi ifade edildi.AA\\n            \"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Economic_Relationship\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "df_train_and_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "iMufCBEqcea8",
        "outputId": "6a39224e-3d99-4d69-9f88-9055f6339553"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                       content\n",
              "Economic_Relationship         \n",
              "0                          273\n",
              "1                          130"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4c1c1581-a247-4e93-af96-6670811a3be8\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>content</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Economic_Relationship</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>130</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4c1c1581-a247-4e93-af96-6670811a3be8')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-4c1c1581-a247-4e93-af96-6670811a3be8 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-4c1c1581-a247-4e93-af96-6670811a3be8');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-110097d5-b9f0-4e48-937b-2ad8ba9c65b9\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-110097d5-b9f0-4e48-937b-2ad8ba9c65b9')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-110097d5-b9f0-4e48-937b-2ad8ba9c65b9 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"df_train_and_val\",\n  \"rows\": 2,\n  \"fields\": [\n    {\n      \"column\": \"Economic_Relationship\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"content\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 101,\n        \"min\": 130,\n        \"max\": 273,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          130,\n          273\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "df_train_and_val.groupby('Economic_Relationship').count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZX1gseeXjD2v"
      },
      "source": [
        "## BERT Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "gkkckLR_cf8F"
      },
      "outputs": [],
      "source": [
        "# Split data into features and labels\n",
        "texts = df_train_and_val['content'].tolist()\n",
        "labels = df_train_and_val['Economic_Relationship'].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mdGSomf9cjKG",
        "outputId": "8be06086-44c7-4396-aa7d-bbcd8cefd052"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('dbmdz/bert-base-turkish-128k-cased')\n",
        "\n",
        "class NewspaperDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len=512):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "            truncation=True\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'text': text,\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "gzMXKq2mclXw"
      },
      "outputs": [],
      "source": [
        "# Define training and evaluation functions\n",
        "def train_epoch(model, data_loader, optimizer, device, n_examples):\n",
        "    model.train()\n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "\n",
        "    for d in data_loader:\n",
        "        input_ids = d['input_ids'].to(device)\n",
        "        attention_mask = d['attention_mask'].to(device)\n",
        "        labels = d['labels'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "\n",
        "        _, preds = torch.max(outputs.logits, dim=1)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        correct_predictions += torch.sum(preds == labels)\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    return correct_predictions.double() / n_examples, sum(losses) / n_examples\n",
        "\n",
        "def eval_model(model, data_loader, device, n_examples):\n",
        "    model.eval()\n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for d in data_loader:\n",
        "            input_ids = d['input_ids'].to(device)\n",
        "            attention_mask = d['attention_mask'].to(device)\n",
        "            labels = d['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "\n",
        "            _, preds = torch.max(outputs.logits, dim=1)\n",
        "            loss = outputs.loss\n",
        "\n",
        "            correct_predictions += torch.sum(preds == labels)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            all_predictions.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    accuracy = correct_predictions.double() / n_examples\n",
        "    avg_loss = sum(losses) / n_examples\n",
        "    class_report = classification_report(all_labels, all_predictions, output_dict=True)\n",
        "    conf_matrix = confusion_matrix(all_labels, all_predictions)\n",
        "\n",
        "    # ✅ Now returns five values including predictions\n",
        "    return accuracy, avg_loss, class_report, conf_matrix, all_predictions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Device setup\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Data split\n",
        "train_idx, val_idx = train_test_split(\n",
        "    list(range(len(texts))), test_size=0.3, stratify=labels, random_state=2\n",
        ")\n",
        "train_texts = [texts[i] for i in train_idx]\n",
        "val_texts = [texts[i] for i in val_idx]\n",
        "train_labels = [labels[i] for i in train_idx]\n",
        "val_labels = [labels[i] for i in val_idx]\n",
        "\n",
        "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix\n",
        "\n",
        "def objective(trial):\n",
        "    # Sample hyperparameters\n",
        "    learning_rate = trial.suggest_float(\"lr\", 1e-5, 5e-5, log=True)\n",
        "    batch_size = trial.suggest_categorical(\"batch_size\", [8, 16, 32, 64])\n",
        "    num_epochs = trial.suggest_int(\"num_epochs\", 3, 8)\n",
        "    seed = trial.suggest_int(\"seed\", 1, 10000)\n",
        "    weight_decay = trial.suggest_float(\"weight_decay\", 0.0, 0.3)\n",
        "\n",
        "    # Set all seeds\n",
        "    import random, os\n",
        "    import numpy as np\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "\n",
        "    # Prepare data\n",
        "    train_dataset = NewspaperDataset(train_texts, train_labels, tokenizer)\n",
        "    val_dataset = NewspaperDataset(val_texts, val_labels, tokenizer)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "\n",
        "    # Initialize model\n",
        "    model = BertForSequenceClassification.from_pretrained(\n",
        "        'dbmdz/bert-base-turkish-128k-cased', num_labels=2\n",
        "    ).to(device)\n",
        "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Train\n",
        "    for epoch in range(num_epochs):\n",
        "        train_epoch(model, train_loader, optimizer, device, len(train_dataset))\n",
        "\n",
        "    # Evaluate\n",
        "    _, _, _, _, val_preds = eval_model(model, val_loader, device, len(val_dataset))\n",
        "    val_true = [val_labels[i] for i in range(len(val_preds))]\n",
        "\n",
        "    # Per-class precision, recall, f1\n",
        "    precision, recall, f1, support = precision_recall_fscore_support(val_true, val_preds, average=None, labels=[0,1])\n",
        "    f1_macro = f1.mean()\n",
        "    f2_class_1 = (5 * precision[1] * recall[1]) / (4 * precision[1] + recall[1] + 1e-10)\n",
        "\n",
        "    # Confusion Matrix\n",
        "    conf = confusion_matrix(val_true, val_preds, labels=[0, 1])\n",
        "    tn, fp, fn, tp = conf.ravel()\n",
        "\n",
        "    # Optional: log metrics as trial attributes (can be accessed later)\n",
        "    trial.set_user_attr(\"precision_0\", precision[0])\n",
        "    trial.set_user_attr(\"recall_0\", recall[0])\n",
        "    trial.set_user_attr(\"precision_1\", precision[1])\n",
        "    trial.set_user_attr(\"recall_1\", recall[1])\n",
        "    trial.set_user_attr(\"f1_macro\", f1_macro)\n",
        "    trial.set_user_attr(\"f2_class_1\", f2_class_1)\n",
        "    trial.set_user_attr(\"conf_matrix\", conf.tolist())  # so it's serializable\n",
        "    trial.set_user_attr(\"seed\", seed)\n",
        "\n",
        "    return f2_class_1  # 👈 changed from (f1_macro, f2_class_1) to only f2_class_1\n",
        "\n",
        "\n",
        "# 🔁 SINGLE-objective study\n",
        "study = optuna.create_study(direction=\"maximize\")\n",
        "study.optimize(objective, n_trials=300)\n",
        "\n",
        "# Show top trial\n",
        "best_trial = study.best_trial\n",
        "print(\"\\n✅ Best Trial (F2-class1):\")\n",
        "print(f\"F2 (class 1): {best_trial.value:.4f}\")\n",
        "print(f\"Precision (0): {best_trial.user_attrs['precision_0']:.4f}, Recall (0): {best_trial.user_attrs['recall_0']:.4f}\")\n",
        "print(f\"Precision (1): {best_trial.user_attrs['precision_1']:.4f}, Recall (1): {best_trial.user_attrs['recall_1']:.4f}\")\n",
        "print(f\"Seed: {best_trial.user_attrs['seed']}\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(np.array(best_trial.user_attrs[\"conf_matrix\"]))\n",
        "\n",
        "# Final model training with best params\n",
        "best_params = best_trial.params\n",
        "\n",
        "final_model = BertForSequenceClassification.from_pretrained(\n",
        "    'dbmdz/bert-base-turkish-128k-cased', num_labels=2\n",
        ").to(device)\n",
        "final_optimizer = AdamW(final_model.parameters(), lr=best_params[\"lr\"])\n",
        "\n",
        "train_dataset = NewspaperDataset(train_texts, train_labels, tokenizer)\n",
        "train_loader = DataLoader(train_dataset, batch_size=best_params[\"batch_size\"], shuffle=True)\n",
        "\n",
        "for epoch in range(best_params[\"num_epochs\"]):\n",
        "    train_epoch(final_model, train_loader, final_optimizer, device, len(train_dataset))\n",
        "\n",
        "torch.save(final_model.state_dict(), 'bert_model_optimized_economicrelationship.pth')\n",
        "print(\"\\n✅ Final model saved as 'bert_model_optimized_economicrelationship.pth'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dp8ErLjf69Z-",
        "outputId": "a35cbed4-a6d5-443d-feb3-324b1902a81d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-18 18:24:13,564] A new study created in memory with name: no-name-76781531-afdd-411c-ada9-0e8330a6bb6e\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 18:24:57,065] Trial 0 finished with value: 0.8883248730758458 and parameters: {'lr': 1.4002065768423477e-05, 'batch_size': 64, 'num_epochs': 8, 'seed': 6004, 'weight_decay': 0.19636998126187685}. Best is trial 0 with value: 0.8883248730758458.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 18:25:28,550] Trial 1 finished with value: 0.9653465346314821 and parameters: {'lr': 1.0456875339872354e-05, 'batch_size': 8, 'num_epochs': 5, 'seed': 4503, 'weight_decay': 0.06870561542682886}. Best is trial 1 with value: 0.9653465346314821.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 18:26:06,142] Trial 2 finished with value: 0.9390862943956428 and parameters: {'lr': 4.5003828057405524e-05, 'batch_size': 64, 'num_epochs': 7, 'seed': 5762, 'weight_decay': 0.012237397694056651}. Best is trial 1 with value: 0.9653465346314821.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 18:26:29,629] Trial 3 finished with value: 0.9558823529186852 and parameters: {'lr': 3.524805467535427e-05, 'batch_size': 16, 'num_epochs': 4, 'seed': 701, 'weight_decay': 0.27070327142380207}. Best is trial 1 with value: 0.9653465346314821.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 18:27:07,057] Trial 4 finished with value: 0.9390862943956428 and parameters: {'lr': 2.10545545302926e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 464, 'weight_decay': 0.05052042515331555}. Best is trial 1 with value: 0.9653465346314821.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 18:27:44,212] Trial 5 finished with value: 0.8593749999809569 and parameters: {'lr': 4.133289647673403e-05, 'batch_size': 64, 'num_epochs': 7, 'seed': 8903, 'weight_decay': 0.26907746908593344}. Best is trial 1 with value: 0.9653465346314821.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 18:28:29,428] Trial 6 finished with value: 0.9183673469184715 and parameters: {'lr': 1.8723579389683546e-05, 'batch_size': 16, 'num_epochs': 8, 'seed': 6579, 'weight_decay': 0.21976156120585777}. Best is trial 1 with value: 0.9653465346314821.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 18:28:51,266] Trial 7 finished with value: 0.6111111110966666 and parameters: {'lr': 1.6419143650671792e-05, 'batch_size': 64, 'num_epochs': 4, 'seed': 3697, 'weight_decay': 0.23204677053942455}. Best is trial 1 with value: 0.9653465346314821.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 18:29:34,635] Trial 8 finished with value: 0.9390862943956428 and parameters: {'lr': 1.2421490487120539e-05, 'batch_size': 8, 'num_epochs': 7, 'seed': 6025, 'weight_decay': 0.040375563242314234}. Best is trial 1 with value: 0.9653465346314821.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 18:30:12,233] Trial 9 finished with value: 0.9296482411848563 and parameters: {'lr': 1.7007723087512184e-05, 'batch_size': 32, 'num_epochs': 7, 'seed': 1649, 'weight_decay': 0.14732911117810013}. Best is trial 1 with value: 0.9653465346314821.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 18:30:31,866] Trial 10 finished with value: 0.9359605911107646 and parameters: {'lr': 2.8629291230998098e-05, 'batch_size': 8, 'num_epochs': 3, 'seed': 3301, 'weight_decay': 0.10957584644578525}. Best is trial 1 with value: 0.9653465346314821.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 18:31:00,787] Trial 11 finished with value: 0.9090909090700184 and parameters: {'lr': 1.0034500045834174e-05, 'batch_size': 16, 'num_epochs': 5, 'seed': 2515, 'weight_decay': 0.10138740786772318}. Best is trial 1 with value: 0.9653465346314821.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 18:31:24,289] Trial 12 finished with value: 0.94999999997855 and parameters: {'lr': 2.8131072808999988e-05, 'batch_size': 16, 'num_epochs': 4, 'seed': 543, 'weight_decay': 0.2912351534697157}. Best is trial 1 with value: 0.9653465346314821.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 18:31:51,724] Trial 13 finished with value: 0.8638743455310299 and parameters: {'lr': 3.120784741998651e-05, 'batch_size': 32, 'num_epochs': 5, 'seed': 8425, 'weight_decay': 0.16957907356001253}. Best is trial 1 with value: 0.9653465346314821.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 18:32:09,849] Trial 14 finished with value: 0.8762886597741258 and parameters: {'lr': 3.358820353023522e-05, 'batch_size': 16, 'num_epochs': 3, 'seed': 4183, 'weight_decay': 0.09686222989202277}. Best is trial 1 with value: 0.9653465346314821.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 18:32:35,401] Trial 15 finished with value: 0.9183673469184715 and parameters: {'lr': 2.5150418952251544e-05, 'batch_size': 8, 'num_epochs': 4, 'seed': 7614, 'weight_decay': 0.13737430921421734}. Best is trial 1 with value: 0.9653465346314821.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 18:33:06,918] Trial 16 finished with value: 0.9183673469184715 and parameters: {'lr': 3.7795119757268894e-05, 'batch_size': 8, 'num_epochs': 5, 'seed': 9985, 'weight_decay': 0.05679482455631589}. Best is trial 1 with value: 0.9653465346314821.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 18:33:41,346] Trial 17 finished with value: 0.8717948717748718 and parameters: {'lr': 1.0071666255241492e-05, 'batch_size': 16, 'num_epochs': 6, 'seed': 2042, 'weight_decay': 0.2502185554528917}. Best is trial 1 with value: 0.9653465346314821.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 18:34:03,632] Trial 18 finished with value: 0.8593749999809569 and parameters: {'lr': 2.2835288920825728e-05, 'batch_size': 32, 'num_epochs': 4, 'seed': 5039, 'weight_decay': 0.19647696873451687}. Best is trial 1 with value: 0.9653465346314821.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 18:34:21,716] Trial 19 finished with value: 0.7219251336725528 and parameters: {'lr': 1.2394002037134816e-05, 'batch_size': 16, 'num_epochs': 3, 'seed': 1357, 'weight_decay': 0.007747327307267925}. Best is trial 1 with value: 0.9653465346314821.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 18:34:59,578] Trial 20 finished with value: 0.8974358974158974 and parameters: {'lr': 3.598406828148646e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 2941, 'weight_decay': 0.0852079412418269}. Best is trial 1 with value: 0.9653465346314821.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 18:35:23,074] Trial 21 finished with value: 0.9359605911107646 and parameters: {'lr': 2.7002122942662632e-05, 'batch_size': 16, 'num_epochs': 4, 'seed': 60, 'weight_decay': 0.2956301795092347}. Best is trial 1 with value: 0.9653465346314821.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 18:35:52,005] Trial 22 finished with value: 0.9296482411848563 and parameters: {'lr': 4.9592871223211327e-05, 'batch_size': 16, 'num_epochs': 5, 'seed': 764, 'weight_decay': 0.29985919631179175}. Best is trial 1 with value: 0.9653465346314821.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 18:36:15,534] Trial 23 finished with value: 0.9278350515267031 and parameters: {'lr': 3.0621415303268355e-05, 'batch_size': 16, 'num_epochs': 4, 'seed': 1025, 'weight_decay': 0.26737879602889136}. Best is trial 1 with value: 0.9653465346314821.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 18:36:39,080] Trial 24 finished with value: 0.9067357512759672 and parameters: {'lr': 2.406650536549868e-05, 'batch_size': 16, 'num_epochs': 4, 'seed': 4418, 'weight_decay': 0.2822905031926861}. Best is trial 1 with value: 0.9653465346314821.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 18:37:08,065] Trial 25 finished with value: 0.9137055837357444 and parameters: {'lr': 2.0171215973145577e-05, 'batch_size': 16, 'num_epochs': 5, 'seed': 2322, 'weight_decay': 0.2427095700118273}. Best is trial 1 with value: 0.9653465346314821.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 18:37:27,693] Trial 26 finished with value: 0.9183673469184715 and parameters: {'lr': 3.8818216520193715e-05, 'batch_size': 8, 'num_epochs': 3, 'seed': 185, 'weight_decay': 0.1954129658938561}. Best is trial 1 with value: 0.9653465346314821.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 18:37:49,986] Trial 27 finished with value: 0.8883248730758458 and parameters: {'lr': 3.399842033690978e-05, 'batch_size': 32, 'num_epochs': 4, 'seed': 1504, 'weight_decay': 0.12032050111343642}. Best is trial 1 with value: 0.9653465346314821.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 18:38:19,239] Trial 28 finished with value: 0.9183673469184715 and parameters: {'lr': 2.6975054846473136e-05, 'batch_size': 16, 'num_epochs': 5, 'seed': 2672, 'weight_decay': 0.0749419033710858}. Best is trial 1 with value: 0.9653465346314821.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 18:38:36,101] Trial 29 finished with value: 0.8593749999809569 and parameters: {'lr': 1.4295077531817339e-05, 'batch_size': 64, 'num_epochs': 3, 'seed': 6864, 'weight_decay': 0.16901866250318168}. Best is trial 1 with value: 0.9653465346314821.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 18:39:13,518] Trial 30 finished with value: 0.8900523560022341 and parameters: {'lr': 4.2597572749395267e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 5249, 'weight_decay': 0.2040222204246302}. Best is trial 1 with value: 0.9653465346314821.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 18:39:55,540] Trial 31 finished with value: 0.9296482411848563 and parameters: {'lr': 4.891733917223936e-05, 'batch_size': 64, 'num_epochs': 8, 'seed': 5611, 'weight_decay': 0.0029212753938865667}. Best is trial 1 with value: 0.9653465346314821.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 18:40:27,399] Trial 32 finished with value: 0.9113300492388435 and parameters: {'lr': 4.434191513418626e-05, 'batch_size': 64, 'num_epochs': 6, 'seed': 6433, 'weight_decay': 0.024396479462214182}. Best is trial 1 with value: 0.9653465346314821.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 18:41:04,393] Trial 33 finished with value: 0.9203980099285288 and parameters: {'lr': 4.599854027990821e-05, 'batch_size': 64, 'num_epochs': 7, 'seed': 7326, 'weight_decay': 0.036451687868915346}. Best is trial 1 with value: 0.9653465346314821.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 18:41:31,259] Trial 34 finished with value: 0.9020618556504143 and parameters: {'lr': 4.010370301434998e-05, 'batch_size': 64, 'num_epochs': 5, 'seed': 4422, 'weight_decay': 0.07302313446321841}. Best is trial 1 with value: 0.9653465346314821.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 18:41:53,197] Trial 35 finished with value: 0.8115183245886215 and parameters: {'lr': 3.168138580762058e-05, 'batch_size': 64, 'num_epochs': 4, 'seed': 932, 'weight_decay': 0.26693196165972166}. Best is trial 1 with value: 0.9653465346314821.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 18:42:38,347] Trial 36 finished with value: 0.9137055837357444 and parameters: {'lr': 2.0762764419542007e-05, 'batch_size': 16, 'num_epochs': 8, 'seed': 3704, 'weight_decay': 0.05907745635921795}. Best is trial 1 with value: 0.9653465346314821.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 18:43:21,672] Trial 37 finished with value: 0.9343434343225436 and parameters: {'lr': 3.572969613883823e-05, 'batch_size': 8, 'num_epochs': 7, 'seed': 8251, 'weight_decay': 0.025358926814243468}. Best is trial 1 with value: 0.9653465346314821.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 18:43:54,040] Trial 38 finished with value: 0.9390862943956428 and parameters: {'lr': 2.9470931011530142e-05, 'batch_size': 64, 'num_epochs': 6, 'seed': 5786, 'weight_decay': 0.22244038604070782}. Best is trial 1 with value: 0.9653465346314821.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 18:44:17,853] Trial 39 finished with value: 0.9313725489971165 and parameters: {'lr': 1.6087831728035186e-05, 'batch_size': 16, 'num_epochs': 4, 'seed': 1926, 'weight_decay': 0.2816924464436242}. Best is trial 1 with value: 0.9653465346314821.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 18:44:55,515] Trial 40 finished with value: 0.9137055837357444 and parameters: {'lr': 1.8760040207972934e-05, 'batch_size': 32, 'num_epochs': 7, 'seed': 551, 'weight_decay': 0.24857774175818928}. Best is trial 1 with value: 0.9653465346314821.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 18:45:33,067] Trial 41 finished with value: 0.9183673469184715 and parameters: {'lr': 1.2898783011202977e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 261, 'weight_decay': 0.05167084113239049}. Best is trial 1 with value: 0.9653465346314821.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 18:46:16,423] Trial 42 finished with value: 0.9137055837357444 and parameters: {'lr': 1.1559952021116125e-05, 'batch_size': 8, 'num_epochs': 7, 'seed': 1011, 'weight_decay': 0.01789943134262023}. Best is trial 1 with value: 0.9653465346314821.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 18:46:47,935] Trial 43 finished with value: 0.9343434343225436 and parameters: {'lr': 2.7031331964228874e-05, 'batch_size': 8, 'num_epochs': 5, 'seed': 3343, 'weight_decay': 0.03300089025026924}. Best is trial 1 with value: 0.9653465346314821.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 18:47:37,217] Trial 44 finished with value: 0.9343434343225436 and parameters: {'lr': 1.499625471744214e-05, 'batch_size': 8, 'num_epochs': 8, 'seed': 1399, 'weight_decay': 0.12355976216078884}. Best is trial 1 with value: 0.9653465346314821.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 18:48:14,651] Trial 45 finished with value: 0.95477386932556 and parameters: {'lr': 3.323379163892147e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 570, 'weight_decay': 0.06762887460932607}. Best is trial 1 with value: 0.9653465346314821.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 18:48:52,279] Trial 46 finished with value: 0.97499999997855 and parameters: {'lr': 3.405099458625822e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 6299, 'weight_decay': 0.17450331385886172}. Best is trial 46 with value: 0.97499999997855.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 18:49:23,751] Trial 47 finished with value: 0.8638743455310299 and parameters: {'lr': 3.248198661078305e-05, 'batch_size': 8, 'num_epochs': 5, 'seed': 6394, 'weight_decay': 0.163319112493437}. Best is trial 46 with value: 0.97499999997855.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 18:50:01,275] Trial 48 finished with value: 0.94999999997855 and parameters: {'lr': 2.877997850987399e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 9793, 'weight_decay': 0.14807542674169705}. Best is trial 46 with value: 0.97499999997855.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 18:50:32,832] Trial 49 finished with value: 0.9137055837357444 and parameters: {'lr': 3.578308054058223e-05, 'batch_size': 8, 'num_epochs': 5, 'seed': 1898, 'weight_decay': 0.09874257925479815}. Best is trial 46 with value: 0.97499999997855.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 18:51:10,503] Trial 50 finished with value: 0.94999999997855 and parameters: {'lr': 2.515085231188838e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 7806, 'weight_decay': 0.13459700604466315}. Best is trial 46 with value: 0.97499999997855.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 18:51:48,196] Trial 51 finished with value: 0.9137055837357444 and parameters: {'lr': 2.9003020523838203e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 9746, 'weight_decay': 0.1483707950002447}. Best is trial 46 with value: 0.97499999997855.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 18:52:25,708] Trial 52 finished with value: 0.9644670050555414 and parameters: {'lr': 3.3588866660143374e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 9225, 'weight_decay': 0.1842813824097665}. Best is trial 46 with value: 0.97499999997855.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 18:53:03,482] Trial 53 finished with value: 0.9390862943956428 and parameters: {'lr': 3.3857987575195624e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 8963, 'weight_decay': 0.20423934798395027}. Best is trial 46 with value: 0.97499999997855.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 18:53:40,922] Trial 54 finished with value: 0.9595959595750688 and parameters: {'lr': 3.6859997729490336e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 595, 'weight_decay': 0.28521191297879334}. Best is trial 46 with value: 0.97499999997855.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 18:54:18,325] Trial 55 finished with value: 0.8762886597741258 and parameters: {'lr': 3.697230982331378e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 7024, 'weight_decay': 0.18186194846654885}. Best is trial 46 with value: 0.97499999997855.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 18:55:02,349] Trial 56 finished with value: 0.9390862943956428 and parameters: {'lr': 3.994981358425644e-05, 'batch_size': 8, 'num_epochs': 7, 'seed': 3884, 'weight_decay': 0.22915348596557072}. Best is trial 46 with value: 0.97499999997855.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 18:55:39,758] Trial 57 finished with value: 0.95477386932556 and parameters: {'lr': 4.2364817230581136e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 9194, 'weight_decay': 0.1820740569081178}. Best is trial 46 with value: 0.97499999997855.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 18:56:17,160] Trial 58 finished with value: 0.95477386932556 and parameters: {'lr': 3.081543589750616e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 8294, 'weight_decay': 0.2790656990681616}. Best is trial 46 with value: 0.97499999997855.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 18:57:00,559] Trial 59 finished with value: 0.8974358974158974 and parameters: {'lr': 3.4224216654513716e-05, 'batch_size': 8, 'num_epochs': 7, 'seed': 3182, 'weight_decay': 0.23959827347765236}. Best is trial 46 with value: 0.97499999997855.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 18:57:27,965] Trial 60 finished with value: 0.94999999997855 and parameters: {'lr': 3.8054549386133854e-05, 'batch_size': 32, 'num_epochs': 5, 'seed': 567, 'weight_decay': 0.25693933170909306}. Best is trial 46 with value: 0.97499999997855.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 18:58:05,389] Trial 61 finished with value: 0.9487179486979486 and parameters: {'lr': 4.221194511535226e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 9290, 'weight_decay': 0.1826156461685963}. Best is trial 46 with value: 0.97499999997855.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 18:58:42,829] Trial 62 finished with value: 0.9595959595750688 and parameters: {'lr': 4.1021511598679354e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 8730, 'weight_decay': 0.16377368405897985}. Best is trial 46 with value: 0.97499999997855.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 18:59:20,207] Trial 63 finished with value: 0.9183673469184715 and parameters: {'lr': 4.511417799991272e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 7862, 'weight_decay': 0.1164446805665506}. Best is trial 46 with value: 0.97499999997855.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 18:59:57,661] Trial 64 finished with value: 0.9296482411848563 and parameters: {'lr': 3.534262286821705e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 8847, 'weight_decay': 0.16237837226929452}. Best is trial 46 with value: 0.97499999997855.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 19:00:29,207] Trial 65 finished with value: 0.904522613044153 and parameters: {'lr': 3.209505742966014e-05, 'batch_size': 8, 'num_epochs': 5, 'seed': 8569, 'weight_decay': 0.15667832753670385}. Best is trial 46 with value: 0.97499999997855.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 19:01:06,632] Trial 66 finished with value: 0.9343434343225436 and parameters: {'lr': 3.299367194232261e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 9437, 'weight_decay': 0.07713177726305817}. Best is trial 46 with value: 0.97499999997855.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 19:01:44,052] Trial 67 finished with value: 0.8854166666476239 and parameters: {'lr': 3.9243253145619474e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 7492, 'weight_decay': 0.134817451314717}. Best is trial 46 with value: 0.97499999997855.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 19:02:15,559] Trial 68 finished with value: 0.9595959595750688 and parameters: {'lr': 4.67013388708121e-05, 'batch_size': 8, 'num_epochs': 5, 'seed': 9, 'weight_decay': 0.21736443308923858}. Best is trial 46 with value: 0.97499999997855.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 19:02:42,996] Trial 69 finished with value: 0.9090909090700184 and parameters: {'lr': 4.5370806921739673e-05, 'batch_size': 32, 'num_epochs': 5, 'seed': 63, 'weight_decay': 0.21488112515369243}. Best is trial 46 with value: 0.97499999997855.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 19:03:11,956] Trial 70 finished with value: 0.9326424870272624 and parameters: {'lr': 4.775145638384826e-05, 'batch_size': 16, 'num_epochs': 5, 'seed': 1221, 'weight_decay': 0.18667108550142802}. Best is trial 46 with value: 0.97499999997855.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 19:03:49,393] Trial 71 finished with value: 0.9374999999765625 and parameters: {'lr': 4.042397852325617e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 374, 'weight_decay': 0.1747215742149823}. Best is trial 46 with value: 0.97499999997855.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 19:04:20,903] Trial 72 finished with value: 0.9296482411848563 and parameters: {'lr': 3.747902313533564e-05, 'batch_size': 8, 'num_epochs': 5, 'seed': 675, 'weight_decay': 0.2900572222367014}. Best is trial 46 with value: 0.97499999997855.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 19:04:58,603] Trial 73 finished with value: 0.9183673469184715 and parameters: {'lr': 4.7577897186305665e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 4615, 'weight_decay': 0.19361261262372587}. Best is trial 46 with value: 0.97499999997855.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 19:05:24,181] Trial 74 finished with value: 0.9020618556504143 and parameters: {'lr': 4.277438531447564e-05, 'batch_size': 8, 'num_epochs': 4, 'seed': 1721, 'weight_decay': 0.20973738944552478}. Best is trial 46 with value: 0.97499999997855.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 19:06:01,528] Trial 75 finished with value: 0.8808290155246717 and parameters: {'lr': 3.500485150869401e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 807, 'weight_decay': 0.27063847330825425}. Best is trial 46 with value: 0.97499999997855.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 19:06:33,072] Trial 76 finished with value: 0.9405940593839575 and parameters: {'lr': 2.2395342086673236e-05, 'batch_size': 8, 'num_epochs': 5, 'seed': 1182, 'weight_decay': 0.2557182005173581}. Best is trial 46 with value: 0.97499999997855.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 19:07:12,856] Trial 77 finished with value: 0.8883248730758458 and parameters: {'lr': 3.081604129114781e-05, 'batch_size': 16, 'num_epochs': 7, 'seed': 257, 'weight_decay': 0.08989055528855139}. Best is trial 46 with value: 0.97499999997855.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 19:07:32,532] Trial 78 finished with value: 0.77540106950143 and parameters: {'lr': 3.8606913402820315e-05, 'batch_size': 8, 'num_epochs': 3, 'seed': 5242, 'weight_decay': 0.06487873670085273}. Best is trial 46 with value: 0.97499999997855.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 19:08:15,900] Trial 79 finished with value: 0.9595959595750688 and parameters: {'lr': 4.115143479522717e-05, 'batch_size': 8, 'num_epochs': 7, 'seed': 22, 'weight_decay': 0.22647067111010868}. Best is trial 46 with value: 0.97499999997855.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 19:08:55,648] Trial 80 finished with value: 0.95477386932556 and parameters: {'lr': 4.3847535133691776e-05, 'batch_size': 16, 'num_epochs': 7, 'seed': 6183, 'weight_decay': 0.22575398382216053}. Best is trial 46 with value: 0.97499999997855.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 19:09:38,918] Trial 81 finished with value: 0.95477386932556 and parameters: {'lr': 4.095749603541007e-05, 'batch_size': 8, 'num_epochs': 7, 'seed': 357, 'weight_decay': 0.17326386112396475}. Best is trial 46 with value: 0.97499999997855.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 19:10:10,410] Trial 82 finished with value: 0.9390862943956428 and parameters: {'lr': 3.648028499986402e-05, 'batch_size': 8, 'num_epochs': 5, 'seed': 74, 'weight_decay': 0.23819193566572722}. Best is trial 46 with value: 0.97499999997855.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 19:10:48,453] Trial 83 finished with value: 0.8974358974158974 and parameters: {'lr': 1.0659521654367137e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 2225, 'weight_decay': 0.21492841317000347}. Best is trial 46 with value: 0.97499999997855.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 19:11:31,855] Trial 84 finished with value: 0.9595959595750688 and parameters: {'lr': 4.6506466403034566e-05, 'batch_size': 8, 'num_epochs': 7, 'seed': 896, 'weight_decay': 0.19122276312659778}. Best is trial 46 with value: 0.97499999997855.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 19:12:21,135] Trial 85 finished with value: 0.9438775510001041 and parameters: {'lr': 4.984262811127431e-05, 'batch_size': 8, 'num_epochs': 8, 'seed': 885, 'weight_decay': 0.20049964881168436}. Best is trial 46 with value: 0.97499999997855.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 19:13:04,491] Trial 86 finished with value: 0.9183673469184715 and parameters: {'lr': 4.7092868485343445e-05, 'batch_size': 8, 'num_epochs': 7, 'seed': 2722, 'weight_decay': 0.1897169838982656}. Best is trial 46 with value: 0.97499999997855.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 19:13:42,150] Trial 87 finished with value: 0.8883248730758458 and parameters: {'lr': 4.3819194821290585e-05, 'batch_size': 32, 'num_epochs': 7, 'seed': 6, 'weight_decay': 0.23376415125971564}. Best is trial 46 with value: 0.97499999997855.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 19:14:07,866] Trial 88 finished with value: 0.9090909090700184 and parameters: {'lr': 1.917713611763068e-05, 'batch_size': 8, 'num_epochs': 4, 'seed': 1182, 'weight_decay': 0.16168477650745555}. Best is trial 46 with value: 0.97499999997855.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 19:14:57,106] Trial 89 finished with value: 0.9452736318190763 and parameters: {'lr': 4.1177769800266136e-05, 'batch_size': 8, 'num_epochs': 8, 'seed': 1680, 'weight_decay': 0.15366385988030654}. Best is trial 46 with value: 0.97499999997855.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 19:15:36,855] Trial 90 finished with value: 0.9137055837357444 and parameters: {'lr': 4.612442172083313e-05, 'batch_size': 16, 'num_epochs': 7, 'seed': 4764, 'weight_decay': 0.1754264937694231}. Best is trial 46 with value: 0.97499999997855.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 19:16:14,269] Trial 91 finished with value: 0.9296482411848563 and parameters: {'lr': 2.9887619002577542e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 546, 'weight_decay': 0.2075743817537742}. Best is trial 46 with value: 0.97499999997855.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 19:16:51,750] Trial 92 finished with value: 0.9137055837357444 and parameters: {'lr': 3.74802134966972e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 743, 'weight_decay': 0.04589522169939195}. Best is trial 46 with value: 0.97499999997855.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 19:17:35,054] Trial 93 finished with value: 0.9137055837357444 and parameters: {'lr': 3.4392088050688253e-05, 'batch_size': 8, 'num_epochs': 7, 'seed': 6755, 'weight_decay': 0.27418155504340946}. Best is trial 46 with value: 0.97499999997855.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 19:18:12,531] Trial 94 finished with value: 0.9798994974662636 and parameters: {'lr': 3.3196642367225064e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 377, 'weight_decay': 0.2619459428288977}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 19:18:44,095] Trial 95 finished with value: 0.9487179486979486 and parameters: {'lr': 3.6397278942679126e-05, 'batch_size': 8, 'num_epochs': 5, 'seed': 1489, 'weight_decay': 0.26113128013991543}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 19:19:16,132] Trial 96 finished with value: 0.9701492537096237 and parameters: {'lr': 4.339248045755888e-05, 'batch_size': 64, 'num_epochs': 6, 'seed': 997, 'weight_decay': 0.24865354812357635}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 19:19:48,189] Trial 97 finished with value: 0.9452736318190763 and parameters: {'lr': 4.290976915839642e-05, 'batch_size': 64, 'num_epochs': 6, 'seed': 363, 'weight_decay': 0.24681867205743688}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 19:20:20,139] Trial 98 finished with value: 0.94999999997855 and parameters: {'lr': 3.942824967248614e-05, 'batch_size': 64, 'num_epochs': 6, 'seed': 928, 'weight_decay': 0.263253409693651}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 19:20:52,139] Trial 99 finished with value: 0.8928571428368388 and parameters: {'lr': 1.7627282974203737e-05, 'batch_size': 64, 'num_epochs': 6, 'seed': 8012, 'weight_decay': 0.21522588754237912}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 19:21:29,255] Trial 100 finished with value: 0.9090909090700184 and parameters: {'lr': 4.8371184548286756e-05, 'batch_size': 64, 'num_epochs': 7, 'seed': 241, 'weight_decay': 0.28612613468724457}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 19:22:07,126] Trial 101 finished with value: 0.9296482411848563 and parameters: {'lr': 4.434200812139223e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 1038, 'weight_decay': 0.2563444656249818}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 19:22:39,123] Trial 102 finished with value: 0.9343434343225436 and parameters: {'lr': 2.765983236289664e-05, 'batch_size': 64, 'num_epochs': 6, 'seed': 588, 'weight_decay': 0.14059446224327551}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 19:23:16,580] Trial 103 finished with value: 0.9390862943956428 and parameters: {'lr': 3.882428747720066e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 8697, 'weight_decay': 0.27389879971532083}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 19:23:48,134] Trial 104 finished with value: 0.9343434343225436 and parameters: {'lr': 4.112739019105729e-05, 'batch_size': 8, 'num_epochs': 5, 'seed': 7096, 'weight_decay': 0.29978121453647905}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 19:24:07,828] Trial 105 finished with value: 0.9390862943956428 and parameters: {'lr': 4.6531473805072886e-05, 'batch_size': 8, 'num_epochs': 3, 'seed': 1335, 'weight_decay': 0.2505437256058744}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 19:24:42,194] Trial 106 finished with value: 0.9296482411848563 and parameters: {'lr': 3.19547468280252e-05, 'batch_size': 16, 'num_epochs': 6, 'seed': 4095, 'weight_decay': 0.22239393771289878}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 19:25:13,746] Trial 107 finished with value: 0.9090909090700184 and parameters: {'lr': 1.3313113271255068e-05, 'batch_size': 8, 'num_epochs': 5, 'seed': 5630, 'weight_decay': 0.2009947324245536}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 19:25:46,262] Trial 108 finished with value: 0.9137055837357444 and parameters: {'lr': 3.51264641711347e-05, 'batch_size': 32, 'num_epochs': 6, 'seed': 9989, 'weight_decay': 0.23586107788492264}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 19:26:29,615] Trial 109 finished with value: 0.9114583333142904 and parameters: {'lr': 2.3351990339628454e-05, 'batch_size': 8, 'num_epochs': 7, 'seed': 703, 'weight_decay': 0.22899771611472497}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 19:26:51,597] Trial 110 finished with value: 0.7486631015869913 and parameters: {'lr': 4.194729245129022e-05, 'batch_size': 64, 'num_epochs': 4, 'seed': 9702, 'weight_decay': 0.24238873649861012}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 19:27:29,061] Trial 111 finished with value: 0.9390862943956428 and parameters: {'lr': 3.36257694032549e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 482, 'weight_decay': 0.2791519018325139}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 19:28:06,516] Trial 112 finished with value: 0.9595959595750688 and parameters: {'lr': 3.768004459208265e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 9038, 'weight_decay': 0.1665967466227968}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 19:28:43,908] Trial 113 finished with value: 0.9183673469184715 and parameters: {'lr': 3.7295092016201246e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 8974, 'weight_decay': 0.17998657427194073}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 19:29:21,369] Trial 114 finished with value: 0.9438775510001041 and parameters: {'lr': 3.9980695540621755e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 9297, 'weight_decay': 0.16955901034103987}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 19:29:58,755] Trial 115 finished with value: 0.9595959595750688 and parameters: {'lr': 3.838010055015737e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 9601, 'weight_decay': 0.18861569644496431}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 19:30:36,764] Trial 116 finished with value: 0.8928571428368388 and parameters: {'lr': 4.347937148293629e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 9634, 'weight_decay': 0.1651936591457332}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 19:31:14,242] Trial 117 finished with value: 0.9390862943956428 and parameters: {'lr': 3.807124341376389e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 9102, 'weight_decay': 0.19032265595991074}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 19:31:45,702] Trial 118 finished with value: 0.9137055837357444 and parameters: {'lr': 2.5835883017777397e-05, 'batch_size': 8, 'num_epochs': 5, 'seed': 9462, 'weight_decay': 0.19670992613799831}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 19:32:23,158] Trial 119 finished with value: 0.9020618556504143 and parameters: {'lr': 4.524652010496337e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 8476, 'weight_decay': 0.14345532335500644}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 19:33:06,471] Trial 120 finished with value: 0.9137055837357444 and parameters: {'lr': 4.152920542510491e-05, 'batch_size': 8, 'num_epochs': 7, 'seed': 8762, 'weight_decay': 0.15895748655700231}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 19:33:43,859] Trial 121 finished with value: 0.9798994974662636 and parameters: {'lr': 3.2961329798738176e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 9498, 'weight_decay': 0.29190087938994147}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 19:34:21,251] Trial 122 finished with value: 0.95477386932556 and parameters: {'lr': 3.322604276767861e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 9470, 'weight_decay': 0.2935152761444188}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 19:34:58,737] Trial 123 finished with value: 0.9452736318190763 and parameters: {'lr': 3.658562453656389e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 9561, 'weight_decay': 0.15217064485002316}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 19:35:36,179] Trial 124 finished with value: 0.8883248730758458 and parameters: {'lr': 3.550270938015399e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 9083, 'weight_decay': 0.17793401160706113}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 19:36:13,622] Trial 125 finished with value: 0.9595959595750688 and parameters: {'lr': 3.938879802626119e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 8161, 'weight_decay': 0.18595003235812652}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 19:36:51,127] Trial 126 finished with value: 0.904522613044153 and parameters: {'lr': 3.007412694546446e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 9806, 'weight_decay': 0.1291335288686695}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 19:37:22,627] Trial 127 finished with value: 0.8974358974158974 and parameters: {'lr': 3.170335276495408e-05, 'batch_size': 8, 'num_epochs': 5, 'seed': 9299, 'weight_decay': 0.1661114551790498}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 19:38:00,069] Trial 128 finished with value: 0.904522613044153 and parameters: {'lr': 3.44187048580227e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 8683, 'weight_decay': 0.2085155849338097}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 19:38:37,586] Trial 129 finished with value: 0.94999999997855 and parameters: {'lr': 4.8566897787706315e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 106, 'weight_decay': 0.2879825956198766}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 19:39:09,114] Trial 130 finished with value: 0.8928571428368388 and parameters: {'lr': 3.273662231898311e-05, 'batch_size': 8, 'num_epochs': 5, 'seed': 9873, 'weight_decay': 0.1939069783444335}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 19:39:46,660] Trial 131 finished with value: 0.8201058200878055 and parameters: {'lr': 3.95912598926015e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 8412, 'weight_decay': 0.18767336156509176}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 19:40:24,123] Trial 132 finished with value: 0.9183673469184715 and parameters: {'lr': 3.8064700715592436e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 8093, 'weight_decay': 0.1851143747602022}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 19:41:01,577] Trial 133 finished with value: 0.94999999997855 and parameters: {'lr': 4.100830062040966e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 9037, 'weight_decay': 0.17434723215911957}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 19:41:38,991] Trial 134 finished with value: 0.9296482411848563 and parameters: {'lr': 3.597953631827763e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 8824, 'weight_decay': 0.2180912523967553}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 19:42:17,051] Trial 135 finished with value: 0.9343434343225436 and parameters: {'lr': 4.47352220438524e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 7560, 'weight_decay': 0.2660610897350924}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 19:43:00,375] Trial 136 finished with value: 0.8974358974158974 and parameters: {'lr': 4.289802706396617e-05, 'batch_size': 8, 'num_epochs': 7, 'seed': 250, 'weight_decay': 0.17150047352291553}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 19:43:37,859] Trial 137 finished with value: 0.9230769230569231 and parameters: {'lr': 3.8867942481110746e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 8291, 'weight_decay': 0.19994472920881567}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 19:44:09,815] Trial 138 finished with value: 0.8638743455310299 and parameters: {'lr': 1.5449353576664563e-05, 'batch_size': 64, 'num_epochs': 6, 'seed': 414, 'weight_decay': 0.1826517546201794}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 19:44:53,355] Trial 139 finished with value: 0.94999999997855 and parameters: {'lr': 4.629640340333617e-05, 'batch_size': 8, 'num_epochs': 7, 'seed': 5247, 'weight_decay': 0.2841016388805}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 19:45:25,888] Trial 140 finished with value: 0.8928571428368388 and parameters: {'lr': 2.1482233729495985e-05, 'batch_size': 32, 'num_epochs': 6, 'seed': 6087, 'weight_decay': 0.19229958181054826}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 19:45:49,474] Trial 141 finished with value: 0.9390862943956428 and parameters: {'lr': 3.691026616518729e-05, 'batch_size': 16, 'num_epochs': 4, 'seed': 827, 'weight_decay': 0.2703620311185604}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 19:46:23,852] Trial 142 finished with value: 0.9343434343225436 and parameters: {'lr': 3.514827173811267e-05, 'batch_size': 16, 'num_epochs': 6, 'seed': 9275, 'weight_decay': 0.2780727000722258}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 19:47:01,366] Trial 143 finished with value: 0.9390862943956428 and parameters: {'lr': 1.162618029209692e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 1080, 'weight_decay': 0.2524483555502032}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 19:47:38,796] Trial 144 finished with value: 0.94999999997855 and parameters: {'lr': 4.010780297017801e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 127, 'weight_decay': 0.29379467163987233}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 19:48:16,288] Trial 145 finished with value: 0.8974358974158974 and parameters: {'lr': 3.440695028426153e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 3313, 'weight_decay': 0.2625984383076049}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 19:48:35,985] Trial 146 finished with value: 0.8974358974158974 and parameters: {'lr': 3.813245101091808e-05, 'batch_size': 8, 'num_epochs': 3, 'seed': 679, 'weight_decay': 0.10615087330676122}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 19:49:10,356] Trial 147 finished with value: 0.9390862943956428 and parameters: {'lr': 3.1118268330553424e-05, 'batch_size': 16, 'num_epochs': 6, 'seed': 350, 'weight_decay': 0.15670279798895187}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 19:49:52,576] Trial 148 finished with value: 0.8717948717748718 and parameters: {'lr': 4.186861493946126e-05, 'batch_size': 64, 'num_epochs': 8, 'seed': 890, 'weight_decay': 0.16835619147540026}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 19:50:30,040] Trial 149 finished with value: 0.94999999997855 and parameters: {'lr': 3.643127601933987e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 7356, 'weight_decay': 0.24334713533182814}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 19:51:01,524] Trial 150 finished with value: 0.9438775510001041 and parameters: {'lr': 4.9914813365631564e-05, 'batch_size': 8, 'num_epochs': 5, 'seed': 32, 'weight_decay': 0.2056229861823784}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 19:51:38,961] Trial 151 finished with value: 0.9024390243675073 and parameters: {'lr': 3.302874907284294e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 572, 'weight_decay': 0.18071278857713063}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 19:52:16,422] Trial 152 finished with value: 0.9343434343225436 and parameters: {'lr': 3.2525560053710206e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 467, 'weight_decay': 0.058171444111466845}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 19:52:59,771] Trial 153 finished with value: 0.9595959595750688 and parameters: {'lr': 3.376845477672529e-05, 'batch_size': 8, 'num_epochs': 7, 'seed': 1294, 'weight_decay': 0.25767151638268954}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 19:53:43,693] Trial 154 finished with value: 0.9090909090700184 and parameters: {'lr': 3.3796266222765844e-05, 'batch_size': 8, 'num_epochs': 7, 'seed': 1363, 'weight_decay': 0.2605696164167741}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 19:54:27,025] Trial 155 finished with value: 0.9343434343225436 and parameters: {'lr': 3.717227089988473e-05, 'batch_size': 8, 'num_epochs': 7, 'seed': 6524, 'weight_decay': 0.2480951696299493}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 19:55:10,348] Trial 156 finished with value: 0.9183673469184715 and parameters: {'lr': 3.561158305363521e-05, 'batch_size': 8, 'num_epochs': 7, 'seed': 1065, 'weight_decay': 0.2701886427936041}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 19:55:53,689] Trial 157 finished with value: 0.9296482411848563 and parameters: {'lr': 4.3954482169310405e-05, 'batch_size': 8, 'num_epochs': 7, 'seed': 1896, 'weight_decay': 0.2994959735651126}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 19:56:37,033] Trial 158 finished with value: 0.9595959595750688 and parameters: {'lr': 3.9333015333348846e-05, 'batch_size': 8, 'num_epochs': 7, 'seed': 8901, 'weight_decay': 0.2754126254030004}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 19:57:20,349] Trial 159 finished with value: 0.9452736318190763 and parameters: {'lr': 4.003419328874115e-05, 'batch_size': 8, 'num_epochs': 7, 'seed': 8907, 'weight_decay': 0.2813488947647746}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 19:58:03,708] Trial 160 finished with value: 0.9137055837357444 and parameters: {'lr': 3.947408731875674e-05, 'batch_size': 8, 'num_epochs': 7, 'seed': 8584, 'weight_decay': 0.2763763786542123}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 19:58:47,072] Trial 161 finished with value: 0.97499999997855 and parameters: {'lr': 3.8801695347712735e-05, 'batch_size': 8, 'num_epochs': 7, 'seed': 3583, 'weight_decay': 0.2897631674053842}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 19:59:30,457] Trial 162 finished with value: 0.9438775510001041 and parameters: {'lr': 3.8142346308755234e-05, 'batch_size': 8, 'num_epochs': 7, 'seed': 3598, 'weight_decay': 0.2890475135130502}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 20:00:13,821] Trial 163 finished with value: 0.9296482411848563 and parameters: {'lr': 4.1268183167500426e-05, 'batch_size': 8, 'num_epochs': 7, 'seed': 3086, 'weight_decay': 0.2857349022738752}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 20:00:57,153] Trial 164 finished with value: 0.9595959595750688 and parameters: {'lr': 4.2726279595142276e-05, 'batch_size': 8, 'num_epochs': 7, 'seed': 9425, 'weight_decay': 0.2572229219495891}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 20:01:40,670] Trial 165 finished with value: 0.9390862943956428 and parameters: {'lr': 4.7231155186948194e-05, 'batch_size': 8, 'num_epochs': 7, 'seed': 4412, 'weight_decay': 0.231826491626026}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 20:02:24,579] Trial 166 finished with value: 0.9390862943956428 and parameters: {'lr': 3.911383103626237e-05, 'batch_size': 8, 'num_epochs': 7, 'seed': 9176, 'weight_decay': 0.2937158753706849}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 20:03:08,011] Trial 167 finished with value: 0.9438775510001041 and parameters: {'lr': 4.5536697770094565e-05, 'batch_size': 8, 'num_epochs': 7, 'seed': 2454, 'weight_decay': 0.18676382635368907}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 20:03:39,866] Trial 168 finished with value: 0.9452736318190763 and parameters: {'lr': 3.460475287621707e-05, 'batch_size': 8, 'num_epochs': 5, 'seed': 4940, 'weight_decay': 0.2736688647933299}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 20:04:23,891] Trial 169 finished with value: 0.9296482411848563 and parameters: {'lr': 3.732638566823513e-05, 'batch_size': 8, 'num_epochs': 7, 'seed': 4174, 'weight_decay': 0.17707467911994584}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 20:05:01,001] Trial 170 finished with value: 0.8461538461338461 and parameters: {'lr': 1.7716918211849343e-05, 'batch_size': 64, 'num_epochs': 7, 'seed': 3887, 'weight_decay': 0.1631282130055967}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 20:05:44,423] Trial 171 finished with value: 0.8808290155246717 and parameters: {'lr': 4.307403777889328e-05, 'batch_size': 8, 'num_epochs': 7, 'seed': 9530, 'weight_decay': 0.25484124497191774}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 20:06:27,807] Trial 172 finished with value: 0.9438775510001041 and parameters: {'lr': 4.0774160603485764e-05, 'batch_size': 8, 'num_epochs': 7, 'seed': 9355, 'weight_decay': 0.2585930629069887}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 20:07:11,181] Trial 173 finished with value: 0.9390862943956428 and parameters: {'lr': 4.2294324511638303e-05, 'batch_size': 8, 'num_epochs': 7, 'seed': 9062, 'weight_decay': 0.2838217615267561}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 20:07:54,714] Trial 174 finished with value: 0.9137055837357444 and parameters: {'lr': 4.3818885492563896e-05, 'batch_size': 8, 'num_epochs': 7, 'seed': 9688, 'weight_decay': 0.2673909319360962}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 20:08:32,124] Trial 175 finished with value: 0.9390862943956428 and parameters: {'lr': 3.9060976983732815e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 8856, 'weight_decay': 0.22266539843566585}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 20:09:15,508] Trial 176 finished with value: 0.9137055837357444 and parameters: {'lr': 3.610404104483361e-05, 'batch_size': 8, 'num_epochs': 7, 'seed': 9457, 'weight_decay': 0.24119573740662553}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 20:09:48,055] Trial 177 finished with value: 0.9020618556504143 and parameters: {'lr': 4.247780438810102e-05, 'batch_size': 32, 'num_epochs': 6, 'seed': 5582, 'weight_decay': 0.19900907513013172}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 20:10:25,452] Trial 178 finished with value: 0.9183673469184715 and parameters: {'lr': 4.021095374239944e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 9630, 'weight_decay': 0.2108081189185828}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 20:11:03,119] Trial 179 finished with value: 0.8928571428368388 and parameters: {'lr': 4.479278365055207e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 9966, 'weight_decay': 0.1706260847900016}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 20:11:46,618] Trial 180 finished with value: 0.9067357512759672 and parameters: {'lr': 3.197446658171214e-05, 'batch_size': 8, 'num_epochs': 7, 'seed': 8644, 'weight_decay': 0.24927817935712993}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 20:12:20,959] Trial 181 finished with value: 0.9020618556504143 and parameters: {'lr': 3.349505429298381e-05, 'batch_size': 16, 'num_epochs': 6, 'seed': 246, 'weight_decay': 0.26517464923856315}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 20:12:58,410] Trial 182 finished with value: 0.8928571428368388 and parameters: {'lr': 3.506743696563611e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 731, 'weight_decay': 0.2781848940013184}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 20:13:35,839] Trial 183 finished with value: 0.8974358974158974 and parameters: {'lr': 3.771791431753762e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 9071, 'weight_decay': 0.2728357116627443}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 20:14:01,436] Trial 184 finished with value: 0.9230769230569231 and parameters: {'lr': 3.6380636780702346e-05, 'batch_size': 8, 'num_epochs': 4, 'seed': 1221, 'weight_decay': 0.25918639689497996}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 20:14:38,486] Trial 185 finished with value: 0.95477386932556 and parameters: {'lr': 3.851510044222429e-05, 'batch_size': 64, 'num_epochs': 7, 'seed': 9292, 'weight_decay': 0.1910963399476319}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 20:15:15,919] Trial 186 finished with value: 0.9230769230569231 and parameters: {'lr': 4.114469298755408e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 1552, 'weight_decay': 0.2896201507605942}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 20:15:53,375] Trial 187 finished with value: 0.9595959595750688 and parameters: {'lr': 3.03383302151375e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 896, 'weight_decay': 0.01069649024902266}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 20:16:24,902] Trial 188 finished with value: 0.8684210526132132 and parameters: {'lr': 3.086709532986867e-05, 'batch_size': 8, 'num_epochs': 5, 'seed': 940, 'weight_decay': 0.01980533851718822}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 20:17:02,436] Trial 189 finished with value: 0.9390862943956428 and parameters: {'lr': 2.9929369186492982e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 6280, 'weight_decay': 0.17626543865747468}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 20:17:39,847] Trial 190 finished with value: 0.9390862943956428 and parameters: {'lr': 3.232697532201254e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 7777, 'weight_decay': 0.2361108164449739}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 20:18:17,262] Trial 191 finished with value: 0.95477386932556 and parameters: {'lr': 3.396163269098421e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 492, 'weight_decay': 0.012168982261770475}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 20:18:51,690] Trial 192 finished with value: 0.9183673469184715 and parameters: {'lr': 3.5101997749298005e-05, 'batch_size': 16, 'num_epochs': 6, 'seed': 5893, 'weight_decay': 0.08026231299295766}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 20:19:29,128] Trial 193 finished with value: 0.9137055837357444 and parameters: {'lr': 2.8391364548877115e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 946, 'weight_decay': 0.02793194252402077}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 20:20:06,559] Trial 194 finished with value: 0.9296482411848563 and parameters: {'lr': 3.668073928012859e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 2752, 'weight_decay': 0.18381062767956374}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 20:20:55,924] Trial 195 finished with value: 0.9230769230569231 and parameters: {'lr': 4.690407525027457e-05, 'batch_size': 8, 'num_epochs': 8, 'seed': 308, 'weight_decay': 0.2647220873555192}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 20:21:39,221] Trial 196 finished with value: 0.9343434343225436 and parameters: {'lr': 3.319864671785882e-05, 'batch_size': 8, 'num_epochs': 7, 'seed': 790, 'weight_decay': 0.28124590234692587}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 20:22:04,809] Trial 197 finished with value: 0.8920187793182238 and parameters: {'lr': 4.037691509052539e-05, 'batch_size': 8, 'num_epochs': 4, 'seed': 4611, 'weight_decay': 0.04799489356776475}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 20:22:42,250] Trial 198 finished with value: 0.9701492537096237 and parameters: {'lr': 3.800806520114267e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 641, 'weight_decay': 0.005852777702792996}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 20:23:19,813] Trial 199 finished with value: 0.9343434343225436 and parameters: {'lr': 3.862359466277722e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 668, 'weight_decay': 0.000509115334312563}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 20:23:57,264] Trial 200 finished with value: 0.9343434343225436 and parameters: {'lr': 4.194486523857907e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 48, 'weight_decay': 0.012032258111761972}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 20:24:34,707] Trial 201 finished with value: 0.9278350515267031 and parameters: {'lr': 3.772912746074558e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 361, 'weight_decay': 0.0067124767185748726}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 20:25:12,182] Trial 202 finished with value: 0.8717948717748718 and parameters: {'lr': 3.5387261184137196e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 1120, 'weight_decay': 0.29761419163250524}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 20:25:49,671] Trial 203 finished with value: 0.9137055837357444 and parameters: {'lr': 3.4209573388084e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 539, 'weight_decay': 0.01908158524265808}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 20:26:27,148] Trial 204 finished with value: 0.8974358974158974 and parameters: {'lr': 3.6919068880165044e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 787, 'weight_decay': 0.2539952174487059}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 20:27:04,786] Trial 205 finished with value: 0.9020618556504143 and parameters: {'lr': 3.9555964509840434e-05, 'batch_size': 32, 'num_epochs': 7, 'seed': 187, 'weight_decay': 0.1650608408868589}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 20:27:42,225] Trial 206 finished with value: 0.9343434343225436 and parameters: {'lr': 3.151872020941574e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 9258, 'weight_decay': 0.0059811438134006745}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 20:28:19,781] Trial 207 finished with value: 0.9438775510001041 and parameters: {'lr': 4.3319351657557914e-05, 'batch_size': 64, 'num_epochs': 7, 'seed': 8423, 'weight_decay': 0.15822292233725951}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 20:28:54,197] Trial 208 finished with value: 0.9405940593839575 and parameters: {'lr': 3.5806172161731725e-05, 'batch_size': 16, 'num_epochs': 6, 'seed': 8876, 'weight_decay': 0.03778207709530331}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 20:29:31,671] Trial 209 finished with value: 0.9701492537096237 and parameters: {'lr': 1.4264533134544649e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 9527, 'weight_decay': 0.2722127646706441}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 20:30:03,193] Trial 210 finished with value: 0.9390862943956428 and parameters: {'lr': 1.1638703684139388e-05, 'batch_size': 8, 'num_epochs': 5, 'seed': 9724, 'weight_decay': 0.09102070380517639}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 20:30:40,669] Trial 211 finished with value: 0.9183673469184715 and parameters: {'lr': 1.0494396127189054e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 9478, 'weight_decay': 0.27139324949762544}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 20:31:18,691] Trial 212 finished with value: 0.9137055837357444 and parameters: {'lr': 1.4297883053013637e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 9377, 'weight_decay': 0.2689102727418618}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 20:31:56,139] Trial 213 finished with value: 0.9183673469184715 and parameters: {'lr': 1.077753114276112e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 593, 'weight_decay': 0.27842586456233454}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 20:32:33,703] Trial 214 finished with value: 0.9701492537096237 and parameters: {'lr': 1.36819875505546e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 9149, 'weight_decay': 0.26225551578872713}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 20:33:11,245] Trial 215 finished with value: 0.9390862943956428 and parameters: {'lr': 1.3746161055354156e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 9123, 'weight_decay': 0.2460394402737594}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 20:33:48,675] Trial 216 finished with value: 0.9296482411848563 and parameters: {'lr': 2.0319868051795652e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 9588, 'weight_decay': 0.2600214480122364}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 20:34:26,101] Trial 217 finished with value: 0.9137055837357444 and parameters: {'lr': 1.5451107072584555e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 8979, 'weight_decay': 0.28645449454624483}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 20:35:03,498] Trial 218 finished with value: 0.9343434343225436 and parameters: {'lr': 4.564563526895121e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 9840, 'weight_decay': 0.06709422200708838}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 20:35:41,341] Trial 219 finished with value: 0.8974358974158974 and parameters: {'lr': 1.687152515146436e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 9222, 'weight_decay': 0.25344757662335854}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 20:36:18,840] Trial 220 finished with value: 0.9653465346314821 and parameters: {'lr': 2.409444351653579e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 6761, 'weight_decay': 0.19451788809154857}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 20:36:56,284] Trial 221 finished with value: 0.9090909090700184 and parameters: {'lr': 1.282044971326386e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 6785, 'weight_decay': 0.18643264650965374}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 20:37:33,761] Trial 222 finished with value: 0.8974358974158974 and parameters: {'lr': 2.3582312445171114e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 5034, 'weight_decay': 0.19621265178203753}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 20:38:11,198] Trial 223 finished with value: 0.9343434343225436 and parameters: {'lr': 1.2162699058306267e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 8122, 'weight_decay': 0.1787562587029636}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 20:38:48,830] Trial 224 finished with value: 0.9183673469184715 and parameters: {'lr': 4.0508486067343756e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 8801, 'weight_decay': 0.18894273302087336}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 20:39:26,286] Trial 225 finished with value: 0.9701492537096237 and parameters: {'lr': 2.6842359573787657e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 6972, 'weight_decay': 0.17264772876723758}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 20:40:03,799] Trial 226 finished with value: 0.9137055837357444 and parameters: {'lr': 2.2072936022195084e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 6901, 'weight_decay': 0.16883739756217483}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 20:40:47,193] Trial 227 finished with value: 0.9137055837357444 and parameters: {'lr': 2.3809579982748953e-05, 'batch_size': 8, 'num_epochs': 7, 'seed': 3531, 'weight_decay': 0.1740010390427405}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 20:41:18,727] Trial 228 finished with value: 0.9343434343225436 and parameters: {'lr': 2.682519704141007e-05, 'batch_size': 8, 'num_epochs': 5, 'seed': 9419, 'weight_decay': 0.194096846461119}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 20:41:56,259] Trial 229 finished with value: 0.9183673469184715 and parameters: {'lr': 1.8204439453593175e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 7140, 'weight_decay': 0.2628415839816792}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 20:42:33,803] Trial 230 finished with value: 0.9090909090700184 and parameters: {'lr': 1.0118674953199195e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 7325, 'weight_decay': 0.1809736384606738}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 20:43:11,288] Trial 231 finished with value: 0.9343434343225436 and parameters: {'lr': 2.6898275261552174e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 9074, 'weight_decay': 0.2915936367913479}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 20:43:48,751] Trial 232 finished with value: 0.94999999997855 and parameters: {'lr': 2.509484604189026e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 9535, 'weight_decay': 0.17389539117281086}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 20:44:26,146] Trial 233 finished with value: 0.9090909090700184 and parameters: {'lr': 3.8067600151344995e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 6407, 'weight_decay': 0.14667761057795606}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 20:45:03,579] Trial 234 finished with value: 0.9644670050555414 and parameters: {'lr': 2.6109982945750917e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 6677, 'weight_decay': 0.1863692252454883}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 20:45:41,527] Trial 235 finished with value: 0.9137055837357444 and parameters: {'lr': 2.435071341438364e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 6877, 'weight_decay': 0.20305784347168795}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 20:46:24,932] Trial 236 finished with value: 0.9183673469184715 and parameters: {'lr': 2.4895288023995223e-05, 'batch_size': 8, 'num_epochs': 7, 'seed': 6294, 'weight_decay': 0.18480205711029402}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 20:46:57,560] Trial 237 finished with value: 0.9137055837357444 and parameters: {'lr': 2.2520664297471528e-05, 'batch_size': 64, 'num_epochs': 6, 'seed': 7200, 'weight_decay': 0.19035976895356566}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 20:47:35,040] Trial 238 finished with value: 0.9343434343225436 and parameters: {'lr': 1.9216264730007193e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 6697, 'weight_decay': 0.179045225653922}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 20:48:18,496] Trial 239 finished with value: 0.904522613044153 and parameters: {'lr': 1.1015149173092805e-05, 'batch_size': 8, 'num_epochs': 7, 'seed': 6648, 'weight_decay': 0.19449674005169088}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 20:48:56,122] Trial 240 finished with value: 0.8883248730758458 and parameters: {'lr': 2.6037500608307342e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 6613, 'weight_decay': 0.17003513026505299}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 20:49:33,676] Trial 241 finished with value: 0.9090909090700184 and parameters: {'lr': 2.7291805637014654e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 9194, 'weight_decay': 0.18158148140753125}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 20:50:11,084] Trial 242 finished with value: 0.9343434343225436 and parameters: {'lr': 3.93567612657773e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 6936, 'weight_decay': 0.27475796620633286}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 20:50:48,571] Trial 243 finished with value: 0.8421052631395292 and parameters: {'lr': 2.7883785690002788e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 5620, 'weight_decay': 0.26561709535140166}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 20:51:26,117] Trial 244 finished with value: 0.9390862943956428 and parameters: {'lr': 2.9013894689130265e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 8709, 'weight_decay': 0.18899638570080649}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 20:52:03,628] Trial 245 finished with value: 0.8928571428368388 and parameters: {'lr': 2.6408893565813033e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 20, 'weight_decay': 0.1630163727530618}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 20:52:41,146] Trial 246 finished with value: 0.904522613044153 and parameters: {'lr': 4.0956182044742805e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 1013, 'weight_decay': 0.2271708203189905}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 20:53:18,563] Trial 247 finished with value: 0.9343434343225436 and parameters: {'lr': 3.0524989237010125e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 1233, 'weight_decay': 0.25786696142259596}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 20:54:02,000] Trial 248 finished with value: 0.9343434343225436 and parameters: {'lr': 3.260918510794144e-05, 'batch_size': 8, 'num_epochs': 7, 'seed': 328, 'weight_decay': 0.24915047156624273}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 20:54:39,469] Trial 249 finished with value: 0.9390862943956428 and parameters: {'lr': 3.863763698881993e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 9757, 'weight_decay': 0.28112896277207267}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 20:55:11,010] Trial 250 finished with value: 0.9183673469184715 and parameters: {'lr': 4.218901823088504e-05, 'batch_size': 8, 'num_epochs': 5, 'seed': 9357, 'weight_decay': 0.21397168623328325}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 20:55:43,549] Trial 251 finished with value: 0.9230769230569231 and parameters: {'lr': 2.5372228112903726e-05, 'batch_size': 32, 'num_epochs': 6, 'seed': 7677, 'weight_decay': 0.20211318316003812}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 20:56:21,065] Trial 252 finished with value: 0.9452736318190763 and parameters: {'lr': 4.834666114903604e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 5912, 'weight_decay': 0.1778813704285847}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 20:57:04,429] Trial 253 finished with value: 0.9183673469184715 and parameters: {'lr': 4.449392031613718e-05, 'batch_size': 8, 'num_epochs': 7, 'seed': 6484, 'weight_decay': 0.26913065674030684}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 20:57:41,848] Trial 254 finished with value: 0.9595959595750688 and parameters: {'lr': 1.4659171917605856e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 788, 'weight_decay': 0.1849561044108694}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 20:58:13,831] Trial 255 finished with value: 0.904522613044153 and parameters: {'lr': 3.344998957521342e-05, 'batch_size': 64, 'num_epochs': 6, 'seed': 8919, 'weight_decay': 0.28821649491163626}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 20:58:57,245] Trial 256 finished with value: 0.9137055837357444 and parameters: {'lr': 3.739578521500353e-05, 'batch_size': 8, 'num_epochs': 7, 'seed': 5443, 'weight_decay': 0.19583878690903467}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 20:59:34,722] Trial 257 finished with value: 0.8974358974158974 and parameters: {'lr': 1.593704077312991e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 3921, 'weight_decay': 0.11484708368790696}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 21:00:12,210] Trial 258 finished with value: 0.9090909090700184 and parameters: {'lr': 3.945516353177183e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 440, 'weight_decay': 0.17232697083784534}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 21:00:43,712] Trial 259 finished with value: 0.9067357512759672 and parameters: {'lr': 2.438312676097923e-05, 'batch_size': 8, 'num_epochs': 5, 'seed': 7931, 'weight_decay': 0.013304034361256456}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 21:01:21,202] Trial 260 finished with value: 0.8928571428368388 and parameters: {'lr': 1.2426961054180755e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 16, 'weight_decay': 0.25966209254208367}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 21:02:04,723] Trial 261 finished with value: 0.9343434343225436 and parameters: {'lr': 3.457132230137051e-05, 'batch_size': 8, 'num_epochs': 7, 'seed': 8538, 'weight_decay': 0.15942280679793733}. Best is trial 94 with value: 0.9798994974662636.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 21:02:42,196] Trial 262 finished with value: 0.9848484848275941 and parameters: {'lr': 4.1141746467727235e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 9522, 'weight_decay': 0.27436221197851735}. Best is trial 262 with value: 0.9848484848275941.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 21:03:19,724] Trial 263 finished with value: 0.9390862943956428 and parameters: {'lr': 4.068619983882405e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 9618, 'weight_decay': 0.27771601406695856}. Best is trial 262 with value: 0.9848484848275941.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 21:04:03,133] Trial 264 finished with value: 0.9390862943956428 and parameters: {'lr': 4.253589035730527e-05, 'batch_size': 8, 'num_epochs': 7, 'seed': 9225, 'weight_decay': 0.27232064839666575}. Best is trial 262 with value: 0.9848484848275941.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 21:04:40,673] Trial 265 finished with value: 0.9137055837357444 and parameters: {'lr': 4.1670466624124606e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 9996, 'weight_decay': 0.26516333090191996}. Best is trial 262 with value: 0.9848484848275941.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 21:05:18,181] Trial 266 finished with value: 0.94999999997855 and parameters: {'lr': 4.401459834251498e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 9485, 'weight_decay': 0.28434667893803345}. Best is trial 262 with value: 0.9848484848275941.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 21:05:50,162] Trial 267 finished with value: 0.9512195121723855 and parameters: {'lr': 3.9993842515703465e-05, 'batch_size': 64, 'num_epochs': 6, 'seed': 9722, 'weight_decay': 0.2542557678770231}. Best is trial 262 with value: 0.9848484848275941.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 21:06:21,717] Trial 268 finished with value: 0.9343434343225436 and parameters: {'lr': 2.323492558371525e-05, 'batch_size': 8, 'num_epochs': 5, 'seed': 9079, 'weight_decay': 0.26913588463179255}. Best is trial 262 with value: 0.9848484848275941.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 21:07:05,101] Trial 269 finished with value: 0.94999999997855 and parameters: {'lr': 3.840163550407505e-05, 'batch_size': 8, 'num_epochs': 7, 'seed': 237, 'weight_decay': 0.29187894758478233}. Best is trial 262 with value: 0.9848484848275941.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 21:07:42,563] Trial 270 finished with value: 0.8928571428368388 and parameters: {'lr': 1.3469936902276537e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 6168, 'weight_decay': 0.18662725955789528}. Best is trial 262 with value: 0.9848484848275941.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 21:08:20,202] Trial 271 finished with value: 0.9644670050555414 and parameters: {'lr': 3.669135095338574e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 9365, 'weight_decay': 0.2766088139894566}. Best is trial 262 with value: 0.9848484848275941.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 21:08:57,683] Trial 272 finished with value: 0.9595959595750688 and parameters: {'lr': 3.610762687052522e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 9306, 'weight_decay': 0.2772411869473917}. Best is trial 262 with value: 0.9848484848275941.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 21:09:35,117] Trial 273 finished with value: 0.9137055837357444 and parameters: {'lr': 3.773975556231324e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 8905, 'weight_decay': 0.28354464180690725}. Best is trial 262 with value: 0.9848484848275941.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 21:10:07,675] Trial 274 finished with value: 0.9343434343225436 and parameters: {'lr': 2.136884259054069e-05, 'batch_size': 32, 'num_epochs': 6, 'seed': 4386, 'weight_decay': 0.16647946287516438}. Best is trial 262 with value: 0.9848484848275941.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 21:10:45,124] Trial 275 finished with value: 0.9137055837357444 and parameters: {'lr': 3.665931868283548e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 9557, 'weight_decay': 0.2973030492634606}. Best is trial 262 with value: 0.9848484848275941.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 21:11:22,549] Trial 276 finished with value: 0.97499999997855 and parameters: {'lr': 3.9251805958074036e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 8280, 'weight_decay': 0.15256895349722588}. Best is trial 262 with value: 0.9848484848275941.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 21:12:00,014] Trial 277 finished with value: 0.9390862943956428 and parameters: {'lr': 3.727342922054498e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 8492, 'weight_decay': 0.14908197982768928}. Best is trial 262 with value: 0.9848484848275941.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 21:12:38,242] Trial 278 finished with value: 0.9020618556504143 and parameters: {'lr': 3.5439393073314075e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 8292, 'weight_decay': 0.15266245517901506}. Best is trial 262 with value: 0.9848484848275941.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 21:13:15,707] Trial 279 finished with value: 0.9067357512759672 and parameters: {'lr': 1.1268286263952187e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 7096, 'weight_decay': 0.2062082511006444}. Best is trial 262 with value: 0.9848484848275941.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 21:13:53,127] Trial 280 finished with value: 0.8762886597741258 and parameters: {'lr': 3.872204565946951e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 8160, 'weight_decay': 0.15293512980446716}. Best is trial 262 with value: 0.9848484848275941.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 21:14:25,069] Trial 281 finished with value: 0.9390862943956428 and parameters: {'lr': 4.020597709984154e-05, 'batch_size': 64, 'num_epochs': 6, 'seed': 508, 'weight_decay': 0.1708525871612304}. Best is trial 262 with value: 0.9848484848275941.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 21:15:02,523] Trial 282 finished with value: 0.9183673469184715 and parameters: {'lr': 3.3776842701888546e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 7417, 'weight_decay': 0.16089059288897953}. Best is trial 262 with value: 0.9848484848275941.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 21:15:39,937] Trial 283 finished with value: 0.9183673469184715 and parameters: {'lr': 4.6033095918464555e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 4746, 'weight_decay': 0.14104587580967332}. Best is trial 262 with value: 0.9848484848275941.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 21:16:11,538] Trial 284 finished with value: 0.9137055837357444 and parameters: {'lr': 3.234106982000126e-05, 'batch_size': 8, 'num_epochs': 5, 'seed': 2053, 'weight_decay': 0.22073604382721118}. Best is trial 262 with value: 0.9848484848275941.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 21:16:48,983] Trial 285 finished with value: 0.9230769230569231 and parameters: {'lr': 3.5927709690956776e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 305, 'weight_decay': 0.17874037622300687}. Best is trial 262 with value: 0.9848484848275941.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 21:17:26,558] Trial 286 finished with value: 0.94999999997855 and parameters: {'lr': 3.8686595287159405e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 9833, 'weight_decay': 0.1989730235782552}. Best is trial 262 with value: 0.9848484848275941.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 21:18:04,094] Trial 287 finished with value: 0.9438775510001041 and parameters: {'lr': 4.1183710991083454e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 9341, 'weight_decay': 0.1916383064349313}. Best is trial 262 with value: 0.9848484848275941.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 21:18:41,539] Trial 288 finished with value: 0.9278350515267031 and parameters: {'lr': 3.698889787979482e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 688, 'weight_decay': 0.17586852821445226}. Best is trial 262 with value: 0.9848484848275941.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 21:19:19,087] Trial 289 finished with value: 0.9390862943956428 and parameters: {'lr': 4.7429029377634036e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 3003, 'weight_decay': 0.16507858351348412}. Best is trial 262 with value: 0.9848484848275941.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 21:19:56,549] Trial 290 finished with value: 0.9183673469184715 and parameters: {'lr': 3.957459206070744e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 8675, 'weight_decay': 0.2384419370406139}. Best is trial 262 with value: 0.9848484848275941.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 21:20:33,982] Trial 291 finished with value: 0.9183673469184715 and parameters: {'lr': 3.444404172880734e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 6659, 'weight_decay': 0.2897983272914899}. Best is trial 262 with value: 0.9848484848275941.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 21:21:11,427] Trial 292 finished with value: 0.9343434343225436 and parameters: {'lr': 3.7395864999273696e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 194, 'weight_decay': 0.15684622167518278}. Best is trial 262 with value: 0.9848484848275941.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 21:21:53,393] Trial 293 finished with value: 0.9137055837357444 and parameters: {'lr': 4.15742908418937e-05, 'batch_size': 64, 'num_epochs': 8, 'seed': 9126, 'weight_decay': 0.18531074113251286}. Best is trial 262 with value: 0.9848484848275941.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 21:22:24,932] Trial 294 finished with value: 0.9020618556504143 and parameters: {'lr': 3.143907686541471e-05, 'batch_size': 8, 'num_epochs': 5, 'seed': 7951, 'weight_decay': 0.043759413254672634}. Best is trial 262 with value: 0.9848484848275941.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 21:23:02,532] Trial 295 finished with value: 0.97499999997855 and parameters: {'lr': 1.2192523305533261e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 9584, 'weight_decay': 0.033060404532019805}. Best is trial 262 with value: 0.9848484848275941.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 21:23:40,017] Trial 296 finished with value: 0.8928571428368388 and parameters: {'lr': 1.1375239920882412e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 9684, 'weight_decay': 0.13018908023203157}. Best is trial 262 with value: 0.9848484848275941.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 21:24:17,532] Trial 297 finished with value: 0.9487179486979486 and parameters: {'lr': 1.2040691672267314e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 9561, 'weight_decay': 0.02827825790727677}. Best is trial 262 with value: 0.9848484848275941.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 21:24:55,007] Trial 298 finished with value: 0.9390862943956428 and parameters: {'lr': 1.3953620724693215e-05, 'batch_size': 8, 'num_epochs': 6, 'seed': 9422, 'weight_decay': 0.05596387886054104}. Best is trial 262 with value: 0.9848484848275941.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2025-05-18 21:25:27,557] Trial 299 finished with value: 0.8762886597741258 and parameters: {'lr': 1.1868215186702521e-05, 'batch_size': 32, 'num_epochs': 6, 'seed': 9797, 'weight_decay': 0.18072854327632604}. Best is trial 262 with value: 0.9848484848275941.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Best Trial (F2-class1):\n",
            "F2 (class 1): 0.9848\n",
            "Precision (0): 1.0000, Recall (0): 0.9634\n",
            "Precision (1): 0.9286, Recall (1): 1.0000\n",
            "Seed: 9522\n",
            "Confusion Matrix:\n",
            "[[79  3]\n",
            " [ 0 39]]\n",
            "\n",
            "✅ Final model saved as 'bert_model_optimized_economicrelationship.pth'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Learning Curve"
      ],
      "metadata": {
        "id": "OzP8EaJ3CriP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from tqdm import tqdm\n",
        "\n",
        "train_sizes = np.linspace(0.1, 0.9, 10)  # 10% to 100%\n",
        "f2_scores = []\n",
        "\n",
        "# Use same test/validation set throughout\n",
        "train_idx, val_idx = train_test_split(list(range(len(texts))), test_size=0.3, stratify=labels, random_state=42)\n",
        "val_texts = [texts[i] for i in val_idx]\n",
        "val_labels = [labels[i] for i in val_idx]\n",
        "val_dataset = NewspaperDataset(val_texts, val_labels, tokenizer)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32)\n",
        "\n",
        "for frac in tqdm(train_sizes):\n",
        "    # Subsample training data\n",
        "    sss = StratifiedShuffleSplit(n_splits=1, train_size=frac, random_state=42)\n",
        "    small_train_idx, _ = next(sss.split([texts[i] for i in train_idx], [labels[i] for i in train_idx]))\n",
        "\n",
        "    sub_train_texts = [texts[train_idx[i]] for i in small_train_idx]\n",
        "    sub_train_labels = [labels[train_idx[i]] for i in small_train_idx]\n",
        "\n",
        "    # Prepare dataset and loader\n",
        "    train_dataset = NewspaperDataset(sub_train_texts, sub_train_labels, tokenizer)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "    # Train simple model (fixed params for fair comparison)\n",
        "    model = BertForSequenceClassification.from_pretrained('dbmdz/bert-base-turkish-128k-cased', num_labels=2).to(device)\n",
        "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "\n",
        "    for epoch in range(4):  # fixed small number of epochs\n",
        "        train_epoch(model, train_loader, optimizer, device, len(train_dataset))\n",
        "\n",
        "    # Evaluate\n",
        "    _, _, _, _, preds = eval_model(model, val_loader, device, len(val_dataset))\n",
        "    true = val_labels\n",
        "    precision, recall, _, _ = precision_recall_fscore_support(true, preds, average=None, labels=[0, 1])\n",
        "    f2 = (5 * precision[1] * recall[1]) / (4 * precision[1] + recall[1] + 1e-10)\n",
        "    f2_scores.append(f2)\n"
      ],
      "metadata": {
        "id": "CP5RvqHyCqqh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a55fbc90-f4e3-4a69-c152-d0057df0708c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/10 [00:00<?, ?it/s]Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            " 10%|█         | 1/10 [00:04<00:44,  5.00s/it]Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            " 20%|██        | 2/10 [00:10<00:44,  5.59s/it]Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            " 30%|███       | 3/10 [00:18<00:45,  6.55s/it]Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            " 40%|████      | 4/10 [00:28<00:47,  7.86s/it]Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            " 50%|█████     | 5/10 [00:40<00:45,  9.17s/it]Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            " 60%|██████    | 6/10 [00:53<00:42, 10.65s/it]Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            " 70%|███████   | 7/10 [01:09<00:37, 12.35s/it]Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            " 80%|████████  | 8/10 [01:27<00:28, 14.03s/it]Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            " 90%|█████████ | 9/10 [01:46<00:15, 15.74s/it]Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100%|██████████| 10/10 [02:07<00:00, 12.77s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(train_sizes * len(train_idx), f2_scores, marker='o')\n",
        "plt.xlabel('Training Set Size')\n",
        "plt.ylabel('F2 Score (class 1)')\n",
        "plt.title('Learning Curve (BERT Fine-tuning)')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5LVOMRuPC1Qg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "outputId": "8be2ccf5-44d6-439f-ddef-9464c23eb4ce"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Performance on the Test Set"
      ],
      "metadata": {
        "id": "mXU_2J2i5XB3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_excel(\"test_set_economic_relationship.xlsx\")[['content', 'Economic_Relationship']]"
      ],
      "metadata": {
        "id": "mHAvCm-v35Ol"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "pS3m24nu6IDC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "7222fdda-9f17-4bfd-8dce-cfabc7e62ee6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               content  Economic_Relationship\n",
              "0    Aycı: Arda giderse Çin'e sponsor oluruz \\n    ...                      1\n",
              "1    Çin heyeti TBMM'de \\n                ANKARA - ...                      1\n",
              "2    Çin borsası 6 ayda yüzde 50 eridi \\n          ...                      0\n",
              "3    İran'dan AB'ye davet \\n                VİYANA ...                      0\n",
              "4    Çin kamu şirketlerinde kâr erozyonu: Yüzde 2,1...                      0\n",
              "..                                                 ...                    ...\n",
              "175  400 milyar dolarlık anlaşma \\n                ...                      0\n",
              "176  Kuzey Kore ile Çin'den yakın iş birliği sözü \\...                      0\n",
              "177  Çin gümrük vergisini indirdi, Türk markalar ha...                      1\n",
              "178  Çin, munzam karşılıkları artırdı \\n           ...                      0\n",
              "179  Trump Panama Kanalı'nı 'geri aldı': Panama \"Ku...                      0\n",
              "\n",
              "[180 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-58b28e59-3b0d-4bc7-88da-0016cd538801\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>content</th>\n",
              "      <th>Economic_Relationship</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Aycı: Arda giderse Çin'e sponsor oluruz \\n    ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Çin heyeti TBMM'de \\n                ANKARA - ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Çin borsası 6 ayda yüzde 50 eridi \\n          ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>İran'dan AB'ye davet \\n                VİYANA ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Çin kamu şirketlerinde kâr erozyonu: Yüzde 2,1...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>175</th>\n",
              "      <td>400 milyar dolarlık anlaşma \\n                ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>176</th>\n",
              "      <td>Kuzey Kore ile Çin'den yakın iş birliği sözü \\...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>177</th>\n",
              "      <td>Çin gümrük vergisini indirdi, Türk markalar ha...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>178</th>\n",
              "      <td>Çin, munzam karşılıkları artırdı \\n           ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>179</th>\n",
              "      <td>Trump Panama Kanalı'nı 'geri aldı': Panama \"Ku...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>180 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-58b28e59-3b0d-4bc7-88da-0016cd538801')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-58b28e59-3b0d-4bc7-88da-0016cd538801 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-58b28e59-3b0d-4bc7-88da-0016cd538801');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-29e8ddda-0a25-4aa9-8fae-a316e55b02e2\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-29e8ddda-0a25-4aa9-8fae-a316e55b02e2')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-29e8ddda-0a25-4aa9-8fae-a316e55b02e2 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_4231277e-b26d-435d-8756-45caa5bcc822\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_4231277e-b26d-435d-8756-45caa5bcc822 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 180,\n  \"fields\": [\n    {\n      \"column\": \"content\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 180,\n        \"samples\": [\n          \"30 milyon $ yat\\u0131r\\u0131mla \\u00c7inli hamileleri giydirecek \\n                YENER KARADEN\\u0130Z\\u0130STANBUL -\\u00a0AB\\u2019li hamile giyim markalar\\u0131na 30 y\\u0131ldan fazla hizmet veren ABT Tekstil\\u2019in bu tecr\\u00fcbesi ile olu\\u015fturdu\\u011fu GeBe markas\\u0131, bug\\u00fcn T\\u00fcrkiye\\u2019de 13 ma\\u011faza, yurt d\\u0131\\u015f\\u0131nda 20 \\u00fclkede 100 noktaya ula\\u015ft\\u0131. Sat\\u0131\\u015f gelirinin y\\u00fczde 70\\u2019ini yurt d\\u0131\\u015f\\u0131ndan elde eden markan\\u0131n Y\\u00f6netim Kurulu Ba\\u015fkan\\u0131 Adnan Bulak, yurtd\\u0131\\u015f\\u0131nda bulunduklar\\u0131 noktan\\u0131n memnuniyet verici oldu\\u011funu belirterek, \\u201cHedefimiz \\u00c7in pazar\\u0131. \\u00c7inli bir grup ile ciddi g\\u00f6r\\u00fc\\u015fmeler i\\u00e7erisindeyiz. \\u00c7in\\u2019de 30 milyon dolarl\\u0131k bir yat\\u0131r\\u0131m b\\u00fct\\u00e7esi ile 3 y\\u0131l i\\u00e7erisinde 65 ma\\u011faza a\\u00e7may\\u0131 planl\\u0131yoruz\\u201d dedi. 1986\\u2019da kurulan ABT Tekstil, ihracata y\\u00f6nelik \\u00fcretim yapma amac\\u0131yla yola koyulmu\\u015f. G\\u00fcn ge\\u00e7tik\\u00e7e m\\u00fc\\u015fteri portf\\u00f6y\\u00fc hamile giyimine kayan \\u015firket, hamile giyimi \\u00fcretiminde uzmanla\\u015ft\\u0131 ve 2005\\u2019te kendi markalar\\u0131 olan GeBe\\u2019yi olu\\u015fturma karar\\u0131 ald\\u0131. Bu alanda ilk kez AVM\\u2019lere giri\\u015f yapan Gebe, \\u015fu anda AVM konumlu 13 ma\\u011fazada hizmet veriyor.\\u00a02014 senesinde Mall of \\u0130stanbul AVM\\u2019de son ma\\u011fazas\\u0131n\\u0131 a\\u00e7an Ge- Be, 2015\\u2019te AnkaMall AVM ma\\u011fazas\\u0131n\\u0131 a\\u00e7acak. Bunun haricinde proje a\\u015famas\\u0131nda iki ma\\u011faza daha bulunuyor.\\u00a0\\u00c7in\\u2019de 3 y\\u0131lda 65 ma\\u011faza a\\u00e7acak\\u00a02013\\u2019\\u00fcn kendileri i\\u00e7in tatmin edici ge\\u00e7ti\\u011fini anlatan ABT Tekstil Y\\u00f6netim Kurulu Ba\\u015fkan\\u0131 Adnan Bulak, yurti\\u00e7i ve yurtd\\u0131\\u015f\\u0131 sat\\u0131\\u015flar\\u0131n\\u0131n 17 milyon TL\\u2019nin \\u00fczerine \\u00e7\\u0131kt\\u0131\\u011f\\u0131n\\u0131 anlatt\\u0131. Bulak, \\u00f6n\\u00fcm\\u00fczdeki d\\u00f6neme dair beklentiler ile ilgili ise \\u201c2015 y\\u0131l\\u0131 i\\u00e7in planlar\\u0131m\\u0131z yine y\\u00fczde 15 b\\u00fcy\\u00fcme g\\u00f6stererek 23,5 milyon TL seviyesinin \\u00fcst\\u00fcne \\u00e7\\u0131kmak\\u201d diye a\\u00e7\\u0131klad\\u0131. \\u015eirketin sat\\u0131\\u015f gelirinin y\\u00fczde 70\\u2019i yurt d\\u0131\\u015f\\u0131ndan sa\\u011flan\\u0131yor. 20 \\u00fclkede 100 sat\\u0131\\u015f noktas\\u0131nda bulunan GeBe\\u2019nin \\u015fimdiki hedefi \\u00c7in. Adnan Bulak, b\\u00f6lgenin rekabet\\u00e7i ve zor bir pazar olmas\\u0131 nedeni ile yerel bir ortak ile hareket edeceklerini belirtti ve \\u015f\\u00f6yle devam etti: \\u201c\\u00c7inli bir grup olan China-Sweden International Investment ile ciddi g\\u00f6r\\u00fc\\u015fme yap\\u0131yoruz. Teknoloji ve gayrimenkul yat\\u0131r\\u0131mlar\\u0131 ger\\u00e7ekle\\u015ftiren bu grup hamile giyimde parekendeye giri\\u015fi bizimle yapmak istiyor. Tasar\\u0131m ve \\u00fcretim k\\u0131sm\\u0131 bize ait olacak, yat\\u0131r\\u0131m\\u0131 ise grup \\u00fcstlenecek. 30 milyon dolarl\\u0131k bir yat\\u0131r\\u0131m b\\u00fct\\u00e7esi ile 3 y\\u0131lda 65 ma\\u011faza planlan\\u0131yor, \\u00c7in\\u2019deki 200 milyon A plus m\\u00fc\\u015fterinin hamile grubuna ula\\u015fma hedefindeyiz.\\u201d Fas, Tunus, Libya, Senegal, Nijerya\\u2019ya sat\\u0131\\u015f yapan \\u015firket, Afrika\\u2019da yay\\u0131l\\u0131yor. \\u015eirket ihracata ba\\u015flad\\u0131\\u011f\\u0131 ABD'de de g\\u00fc\\u00e7lenmeyi hedefliyor.Jasmini ile b\\u00fcy\\u00fck bedene giriyor\\u00a0\\u00c7in i\\u00e7in bebek giyimine de girme karar\\u0131 alan GeBe, T\\u00fcrkiye\\u2019de ise Jasmini markas\\u0131 ile b\\u00fcy\\u00fck bedene girecek. Adnan Bulak, \\u201cT\\u00fcrkiye\\u2019de do\\u011fum yapt\\u0131ktan sonra kilo veremeyen gen\\u00e7 bayanlar, gen\\u00e7 b\\u00fcy\\u00fck beden giyim m\\u00fc\\u015fterilerinin istedikleri gibi modern ve d\\u00fcnya trendlerini yans\\u0131tan giyim \\u00fcr\\u00fcnlerini piyasada bulamamas\\u0131 Jasmini\\u2019yi kurmam\\u0131za sebep oldu. B\\u00fcy\\u00fck beden m\\u00fc\\u015fterisine 2015 \\u015fubat ay\\u0131ndan itibaren, yaz koleksiyonumuz ile ba\\u015flang\\u0131\\u00e7 yaparak GeBe ma\\u011fazalar\\u0131nda hizmet vermeye ba\\u015flayaca\\u011f\\u0131z\\u201d dedi.\\u00a0130 bin hamileye hizmet veriyor\\u00a0Adnan Bulak\\u2019\\u0131n verdi\\u011fi bilgilere g\\u00f6re, T\\u00fcrkiye\\u2019de y\\u0131ll\\u0131k yakla\\u015f\\u0131k 1 milyon 300 bin hamilelik ve do\\u011fum ger\\u00e7ekle\\u015fiyor. Bu kesimin yakla\\u015f\\u0131k y\\u00fczde 10\\u2019luk b\\u00f6l\\u00fcm\\u00fcne hizmet verdiklerini anlatan Bulak, \\u201cBizim m\\u00fc\\u015fteri profilimiz e\\u011fitimli, sosyal olarak kendini geli\\u015ftirmi\\u015f kad\\u0131nlar grubundan olu\\u015fuyor. Bu a\\u00e7\\u0131dan bak\\u0131ld\\u0131\\u011f\\u0131nda 130 bin adetlik m\\u00fc\\u015fteri say\\u0131s\\u0131yla markam\\u0131zla hamile sekt\\u00f6r\\u00fcnde ciddi s\\u00f6z sahibi oldu\\u011fumuz ve piyasay\\u0131 domine etti\\u011fimizi s\\u00f6ylebiliriz\\u201d dedi.\\n            \",\n          \"\\u00c7in'den AB'ye 'ho\\u015fg\\u00f6r\\u00fc' \\u00e7a\\u011fr\\u0131s\\u0131 \\n                PEK\\u0130N - \\u00c7in Ba\\u015fbakan Yard\\u0131mc\\u0131s\\u0131 Li K\\u0131\\u00e7iang, \\u00c7in ile Avrupa Birli\\u011fi'nin stratejik bir bak\\u0131\\u015f a\\u00e7\\u0131s\\u0131yla ortak kazan\\u00e7 ve ho\\u015fg\\u00f6r\\u00fc anlay\\u0131\\u015f\\u0131n\\u0131 art\\u0131rarak, b\\u00fct\\u00fcn alanlar\\u0131 kapsayan i\\u015fbirli\\u011fini derinle\\u015ftirmeleri ve b\\u00f6ylece stratejik ortakl\\u0131k ili\\u015fkilerini yeni bir d\\u00fczeye ta\\u015f\\u0131malar\\u0131 gerekti\\u011fini belirti.Ba\\u015fbakan Yard\\u0131mc\\u0131s\\u0131 Li, Pekin'de d\\u00fczenlenen \\u00c7in-Avrupa Birli\\u011fi Stratejik Ortakl\\u0131k \\u0130li\\u015fkileri Seminerinde yapt\\u0131\\u011f\\u0131 konu\\u015fmada, uluslararas\\u0131 toplumun \\u00f6nemli iki g\\u00fcc\\u00fc olan \\u00c7in ve Avrupa Birli\\u011fi'nin d\\u00fcnya bar\\u0131\\u015f\\u0131n\\u0131n ve i\\u015fbirli\\u011finin geli\\u015ftirilmesinde \\u00f6nemli y\\u00fck\\u00fcml\\u00fcl\\u00fckleri oldu\\u011funu s\\u00f6yledi.\\u00c7in-Avrupa Birli\\u011fi ili\\u015fkilerinin daha ileriye ta\\u015f\\u0131nmas\\u0131n\\u0131n iki taraf\\u0131n temel \\u00e7\\u0131karlar\\u0131na uygun oldu\\u011funu ve bu ili\\u015fkilerin her ge\\u00e7en g\\u00fcn daha k\\u00fcresel bir boyut kazand\\u0131\\u011f\\u0131n\\u0131 savunan Li, kar\\u015f\\u0131l\\u0131kl\\u0131 yat\\u0131r\\u0131m\\u0131n ve ticaretin art\\u0131r\\u0131lmas\\u0131n\\u0131, y\\u00fcksek ve yeni teknoloji, \\u00e7evre koruma ve enerji tasarrufu ile \\\"ye\\u015fil ekonomi\\\" gibi yeni i\\u015fbirli\\u011fi alanlar\\u0131n\\u0131n geni\\u015fletilmesini istedi. Li, iki taraf\\u0131n birbirlerinin temel \\u00e7\\u0131karlar\\u0131n\\u0131 ve kayg\\u0131lar\\u0131n\\u0131 g\\u00f6zeterek, d\\u00fcnya ekonomisinin dengeli ve s\\u00fcrd\\u00fcr\\u00fclebilir bi\\u00e7imde geli\\u015fmesine g\\u00fc\\u00e7 katmas\\u0131 beklentisini ifade etti.\\u00c7in Ba\\u015fbakan Yard\\u0131mc\\u0131s\\u0131, k\\u00fcresel finans krizinin ve d\\u00fcnya ekonomisindeki b\\u00fcy\\u00fck de\\u011fi\\u015fimlerin beraberinde getirdi\\u011fi sorunlara ve f\\u0131rsatlara kar\\u015f\\u0131, stratejik nitelikli ekonomik yap\\u0131sal d\\u00fczenlemeleri h\\u0131zland\\u0131rmak, kaynak tasarrufunu ve \\u00e7evre koruma \\u00e7al\\u0131\\u015fmalar\\u0131n\\u0131 yo\\u011funla\\u015ft\\u0131rmak ve ekonomik b\\u00fcy\\u00fcmenin kalitesini ve verimlili\\u011fini art\\u0131rmak suretiyle uzun vadeli, istikrarl\\u0131 ve h\\u0131zl\\u0131 b\\u00fcy\\u00fcmeyi ger\\u00e7ekle\\u015ftireceklerini s\\u00f6yledi.\\n            \",\n          \"Bakan \\u015eim\\u015fek, Xuren ile bir araya geldi \\n                \\u00a0\\u00a0PEK\\u0130N - Maliye Bakan\\u0131 Mehmet \\u015eim\\u015fek, T\\u00fcrkiye'nin tek \\u00c7in politikas\\u0131 benimsedi\\u011fini belirtti ve iki \\u00fclke ili\\u015fkilerinin kar\\u015f\\u0131l\\u0131kl\\u0131 sayg\\u0131 ve i\\u00e7i\\u015flerine kar\\u0131\\u015fmama ilkeleri \\u00e7er\\u00e7evesinde y\\u00fcr\\u00fcyece\\u011fini s\\u00f6yledi.\\u00c7in Halk Cumhuriyeti Maliye Bakan\\u0131 Xie Xuren'in resmi davetlisi olarak Pekin'de temaslarda bulunan \\u015eim\\u015fek, Xuren ile bir araya geldi. \\u0130ki \\u00fclke heyetlerinin de kat\\u0131ld\\u0131\\u011f\\u0131 g\\u00f6r\\u00fc\\u015fmede, \\u00c7in'in b\\u00fcy\\u00fck bir ba\\u015far\\u0131 hikayesi oldu\\u011funu kaydeden \\u015eim\\u015fek, \\\"\\u00c7in,T\\u00fcrkiye i\\u00e7in \\u00e7ok \\u00f6nemli bir \\u00fclke. T\\u00fcrkiye, tek \\u00c7in politikas\\u0131 benimsemektedir. \\u0130li\\u015fkiler, kar\\u015f\\u0131l\\u0131kl\\u0131 sayg\\u0131, i\\u00e7i\\u015flerine kar\\u0131\\u015fmama ve ortak menfaat ilkeleri \\u00e7er\\u00e7evesinde y\\u00fcr\\u00fcyor. Bundan sonra da b\\u00f6yle y\\u00fcr\\u00fcyecek\\\" dedi.\\u015eim\\u015fek, T\\u00fcrkiye ile \\u00c7in'in \\u00e7ok geni\\u015f alanda i\\u015fbirli\\u011fi yapabileceklerini dile getiren Bakan \\u015eim\\u015fek, bu i\\u015fbirli\\u011fi i\\u00e7in her iki \\u00fclkede de g\\u00fc\\u00e7l\\u00fc bir irade bulundu\\u011funu vurgulad\\u0131. \\u00c7in'e yapt\\u0131\\u011f\\u0131 ziyaretin de ili\\u015fkilerin geli\\u015fmesine d\\u00f6n\\u00fck bir ad\\u0131m oldu\\u011funu belirtti.Maliye Bakan\\u0131 \\u015eim\\u015fek, bas\\u0131n mensuplar\\u0131na g\\u00f6r\\u00fc\\u015fmeyle ilgili yapt\\u0131\\u011f\\u0131 de\\u011ferlendirmede de, \\\"\\u0130ki \\u00fclke aras\\u0131ndaki ileti\\u015fimin g\\u00fc\\u00e7lendirilmesi, s\\u0131cak diyalog ortam\\u0131 ile dostluk kurulmas\\u0131, ticaretin ve yat\\u0131r\\u0131mlar\\u0131n art\\u0131r\\u0131lmas\\u0131\\\" amac\\u0131yla bu ziyareti ger\\u00e7ekle\\u015ftirdiklerini s\\u00f6yledi.\\u00c7in'in her a\\u00e7\\u0131dan \\u00f6nemli bir \\u00fclke oldu\\u011funa i\\u015faret eden \\u015eim\\u015fek, \\\"Cumhurba\\u015fkan\\u0131m\\u0131z\\u0131n ziyareti ile \\u00e7ok g\\u00fczel bir hava yakalad\\u0131k. Daha yak\\u0131n diyalog ile ili\\u015fkilerimizi canl\\u0131 tutaca\\u011f\\u0131z. \\u0130li\\u015fkilerimizi iyi kurgulay\\u0131p g\\u00f6t\\u00fcr\\u00fcrsek, yat\\u0131r\\u0131m boyutuyla da dengeyi sa\\u011flayabiliriz. \\u00c7in, y\\u0131lda 90 milyar dolar yat\\u0131r\\u0131m \\u00e7ekiyor, 50-55 milyar dolar da sermaye ihra\\u00e7 ediyor. 2,2 trilyon dolar d\\u00f6viz rezervi var. Kendilerine (gelin kaynaklar\\u0131n\\u0131z\\u0131n bir k\\u0131sm\\u0131n\\u0131 T\\u00fcrkiye'de de\\u011ferlendirin. T\\u00fcrkiye gibi \\u00fclkelerin gelece\\u011fi parlakt\\u0131r) dedik\\\" a\\u00e7\\u0131klamas\\u0131nda bulundu.Maliye Bakan\\u0131 \\u015eim\\u015fek, temaslar\\u0131 \\u00e7er\\u00e7evesinde \\u00c7inli yat\\u0131r\\u0131mc\\u0131lar ve finans kesimi temsilcileriyle de g\\u00f6r\\u00fc\\u015fmeler yapt\\u0131.\\n            \"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Economic_Relationship\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 1: Extract test texts and labels\n",
        "test_texts = df['content'].tolist()\n",
        "test_labels = df['Economic_Relationship'].tolist()\n",
        "\n",
        "# STEP 2: Create test dataset and loader\n",
        "test_dataset = NewspaperDataset(test_texts, test_labels, tokenizer)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32)\n",
        "\n",
        "# STEP 3: Load the best model\n",
        "best_model = BertForSequenceClassification.from_pretrained(\n",
        "    'dbmdz/bert-base-turkish-128k-cased', num_labels=2\n",
        ").to(device)\n",
        "best_model.load_state_dict(torch.load('bert_model_optimized_economicrelationship.pth'))\n",
        "best_model.eval()\n",
        "\n",
        "# STEP 4: Evaluate\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
        "\n",
        "# Predict\n",
        "_, _, _, _, test_preds = eval_model(best_model, test_loader, device, len(test_dataset))\n",
        "test_true = [test_labels[i] for i in range(len(test_preds))]\n",
        "\n",
        "# STEP 5: Compute metrics\n",
        "acc = accuracy_score(test_true, test_preds)\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(test_true, test_preds, average=None, labels=[0, 1])\n",
        "f2_class_1 = (5 * precision[1] * recall[1]) / (4 * precision[1] + recall[1] + 1e-10)\n",
        "conf = confusion_matrix(test_true, test_preds, labels=[0, 1])\n",
        "\n",
        "# STEP 6: Show results\n",
        "print(\"\\n📈 Test Set Performance:\")\n",
        "print(f\"Accuracy: {acc:.4f}\")\n",
        "print(f\"Precision (class 0): {precision[0]:.4f}, Recall (class 0): {recall[0]:.4f}\")\n",
        "print(f\"Precision (class 1): {precision[1]:.4f}, Recall (class 1): {recall[1]:.4f}\")\n",
        "print(f\"F1 Scores: class 0 = {f1[0]:.4f}, class 1 = {f1[1]:.4f}\")\n",
        "print(f\"F2 (class 1): {f2_class_1:.4f}\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf)"
      ],
      "metadata": {
        "id": "-6oxrvQJ6IAX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55e7774d-5fc7-4413-b824-f2ac3c7a48c6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📈 Test Set Performance:\n",
            "Accuracy: 0.9611\n",
            "Precision (class 0): 0.9829, Recall (class 0): 0.9583\n",
            "Precision (class 1): 0.9206, Recall (class 1): 0.9667\n",
            "F1 Scores: class 0 = 0.9705, class 1 = 0.9431\n",
            "F2 (class 1): 0.9571\n",
            "Confusion Matrix:\n",
            "[[115   5]\n",
            " [  2  58]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving the Model"
      ],
      "metadata": {
        "id": "_p6G7qdPFIix"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "l0RD2xhjfWCP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5541f0bd-2df1-46aa-f2f5-11fe566ffe1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "✅ Copied: bert_model_optimized_economicrelationship.pth\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# 1. Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2. Define the target folder in Google Drive\n",
        "target_folder = '/content/drive/MyDrive/bert_predictions'\n",
        "os.makedirs(target_folder, exist_ok=True)  # create it if it doesn't exist\n",
        "\n",
        "# 3. Define the list of files you want to copy from Colab to Drive\n",
        "files_to_save = [\n",
        "    'bert_model_optimized_economicrelationship.pth',\n",
        "]\n",
        "\n",
        "# 4. Copy files to the Google Drive folder\n",
        "for file in files_to_save:\n",
        "    if os.path.exists(file):\n",
        "        shutil.copy(file, target_folder)\n",
        "        print(f\"✅ Copied: {file}\")\n",
        "    else:\n",
        "        print(f\"❌ File not found: {file}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}